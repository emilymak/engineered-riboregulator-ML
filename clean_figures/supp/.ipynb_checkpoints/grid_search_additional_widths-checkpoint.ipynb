{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: run a grid search to find the best architecture for predicting the on and off target values per toehold. Uses a randomized parameter search to improve time-efficiency. This notebook is intended for dividing up the share of work of grid search. \n",
    "\n",
    "See Part 4 for what parameters to change per run. This same notebook can be used multiple times, so long as those two variables (rand_param_combos and save_file_tag) are changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements \n",
    "# everything MUST be imported and activated in the O2 cluster before running (as a .py file)\n",
    "\n",
    "import os\n",
    "#disable CUDA\n",
    "\n",
    "import platform\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import keras\n",
    "\n",
    "# some visualization imports\n",
    "from keras import activations\n",
    "\n",
    "# various imports for the keras model\n",
    "from keras.layers.core import Permute\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import keras as keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import metrics as metrics\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Conv1D, Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# evaluate performance w/ on and off regression separately \n",
    "from scipy.stats import pearsonr, spearmanr \n",
    "\n",
    "# imports for the grid search and kfold CV\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# data one-hot encoding imports (help from Luis)\n",
    "from pysster.One_Hot_Encoder import One_Hot_Encoder\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load in data. Filter and sample to avoid bias from expiremental errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/'\n",
    "# diff sheets, so need to read i/n \n",
    "file_name = 'newQC_toehold_data.csv'\n",
    "data_df = pd.read_csv(data_dir + file_name,sep=',')\n",
    "print(data_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_cutoff=1.1\n",
    "data_df = data_df[data_df['on_qc'] >= qc_cutoff].reset_index()\n",
    "data_df = data_df[data_df['off_qc'] >= qc_cutoff].reset_index()\n",
    "toehold_seqs = data_df['switch_sequence']\n",
    "seq_len = len(toehold_seqs[0])\n",
    "print('Number of remaining sequences: ', len(data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now downsample data to avoid bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "on_value_bin_labels = np.arange(1000)\n",
    "on_value_bins = pd.cut(data_df['on_value'], bins=1000, labels=on_value_bin_labels)\n",
    "bin_floor_on = math.floor(data_df['on_value'].value_counts(bins=1000).mean())\n",
    "\n",
    "\n",
    "off_value_bin_labels = np.arange(1000)\n",
    "off_value_bins = pd.cut(data_df['off_value'], bins=1000, labels=off_value_bin_labels)\n",
    "bin_floor_off = math.floor(data_df['off_value'].value_counts(bins=1000).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the 1000 bin counts and preventing no more than \n",
    "# the mean number of counts in each bin, then adding all of the indicies\n",
    "# of the bins to a list for the on and off values\n",
    "sample_ids_on = []\n",
    "for bin_label in on_value_bin_labels:\n",
    "    bin_indices = on_value_bins[on_value_bins == bin_label].index\n",
    "    bin_num = bin_indices.size\n",
    "    if bin_num > bin_floor_on:\n",
    "        sample = np.random.choice(bin_indices, size=bin_floor_on, replace=False)\n",
    "    else:\n",
    "        sample = bin_indices\n",
    "    sample_ids_on.append(sample.tolist())  \n",
    "\n",
    "sample_ids_off = []\n",
    "for bin_label in off_value_bin_labels:\n",
    "    bin_indices = off_value_bins[off_value_bins == bin_label].index\n",
    "    bin_num = bin_indices.size\n",
    "    if bin_num > bin_floor_off:\n",
    "        sample = np.random.choice(bin_indices, size=bin_floor_off, replace=False)\n",
    "    else:\n",
    "        sample = bin_indices\n",
    "    sample_ids_off.append(sample.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking down list of lists into one list\n",
    "sample_on = itertools.chain.from_iterable(sample_ids_on)\n",
    "sample_off = itertools.chain.from_iterable(sample_ids_off)\n",
    "\n",
    "# take union of sample_ids_on and sample_ids_off \n",
    "sample_ids_union = set(sample_on).union(sample_off)\n",
    "sub_df = data_df.loc[sample_ids_union].reset_index(drop=True)\n",
    "\n",
    "print('New number of remaining seqs:', len(sub_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameters to match original (in order to not break later code w/ new sampling)\n",
    "data_df = sub_df\n",
    "toehold_seqs = data_df['switch_sequence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Transform Data. One-hot encode sequences and extact target on and off values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alph_letters = 'ATCG'\n",
    "alph = sorted(list(alph_letters))\n",
    "\n",
    "# one-hot encode\n",
    "# modified code from Luis to get correct format for TPOT w/ our nt seq\n",
    "# use pysster (very fast and simple encoding)  \n",
    "one = One_Hot_Encoder(alph_letters)\n",
    "def _get_one_hot_encoding(seq):\n",
    "    one_hot_seq = one.encode(seq)                         \n",
    "    return one_hot_seq\n",
    "\n",
    "# now convert the data into one_hot_encoding \n",
    "input_col_name = 'switch_sequence'#'switch'\n",
    "X = np.stack(\n",
    "    [_get_one_hot_encoding(s) for s in toehold_seqs]).astype(np.float32)\n",
    "\n",
    "print('input shape: ', X.shape)\n",
    "# reformat for CNN \n",
    "alph_len = len(alph)\n",
    "seq_len = len(data_df[input_col_name][0])\n",
    "X = X.reshape(X.shape[0], seq_len, alph_len).astype('float32')\n",
    "print('modified shape: ', X.shape)\n",
    "\n",
    "y_on = np.array(data_df['on_value'].astype(np.float32))\n",
    "y_off = np.array(data_df['off_value'].astype(np.float32))\n",
    "\n",
    "# combine to both targets included\n",
    "# cols 0-1 = on bin, 2-3 = off bin \n",
    "y = np.transpose(np.array([y_on,y_off,]))\n",
    "print('target shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Set-up framework for model. Ensure needed parameters can be varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "def twoheaded_conv1d(conv_layer_parameters, hidden_layers, dropout_rate = 0.2, reg_coeff = 0.0001,learning_rate=0.001, num_features = 59, num_channels = 4): \n",
    "    # num_features = seq length, num_channels = alphabet size (i.e. # nucleotides)\n",
    "    X_in = Input(shape=(num_features,num_channels),dtype='float32')\n",
    "    #(kernel_width, num_filters) = conv_layer_parameters\n",
    "    prior_layer = X_in \n",
    "    for kernel_width, num_filters in conv_layer_parameters:\n",
    "        conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_width, padding='same')(prior_layer) # mimic a kmer\n",
    "        prior_layer = conv_layer\n",
    "    H = Flatten()(prior_layer)\n",
    "    for h in hidden_layers: \n",
    "        H = Dropout(dropout_rate)(H)\n",
    "        H = Dense(h, activation='relu', kernel_regularizer=l2(reg_coeff))(H)\n",
    "    out_on = Dense(1,activation=\"linear\",name='on_output')(H)\n",
    "    out_off = Dense(1, activation='linear', name='off_output')(H)\n",
    "    model = Model(inputs=[X_in], outputs=[out_on, out_off])\n",
    "    #model.summary()\n",
    "    #crossentropy is for probabilities\n",
    "    opt = optimizers.adam(lr = learning_rate)\n",
    "    model.compile(loss={'on_output': 'mse', 'off_output': 'mse'},optimizer=opt,metrics=['mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Enter hyperparameter list from the google doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER your selected list here\n",
    "# REPLACE the nested list w/ your nested list\n",
    "# RUN7 \n",
    "rand_param_combos =  [[5, 3, 10, 5], [10, 10, 10, 5], [15, 5, 10, 5], [5, 15, 10, 5], [20, 20, 10, 5], [10, 3, 10, 5], [5, 20, 10, 5]]\n",
    "\n",
    "# RUN8 #rand_param_combos = [[5, 3, 5, 5, 0.1, 0.0, 0.0005], [3, 3, 10, 10, 0.3, 0.0001, 0.0005], [7, 5, 5, 5, 0.3, 0.0001, 0.0005], [5, 7, 5, 5, 0.3, 0.0001, 0.0005], [3, 3, 10, 10, 0.1, 0.0, 0.001]]\n",
    "\n",
    "# RUN11 #rand_param_combos =  [[7, 3, 5, 5, 0.1, 0.0001, 0.001], [3, 5, 10, 5, 0.3, 0.0, 0.0005], [3, 5, 10, 10, 0.3, 0.0, 0.001], [5, 5, 10, 5, 0.1, 0.0001, 0.001], [5, 5, 15, 5, 0.1, 0.0001, 0.001]]\n",
    "\n",
    "# RUN2 #rand_param_combos = [[3, 3, 10, 5, 0.1, 0.0001, 0.001], [7, 5, 15, 5, 0.1, 0.0, 0.0005], [7, 3, 10, 5, 0.1, 0.0001, 0.001], [3, 3, 10, 5, 0.1, 0.0, 0.0005], [3, 7, 5, 5, 0.3, 0.0, 0.001]]\n",
    "\n",
    "# RUN12 #rand_param_combos =  [[7, 3, 5, 10, 0.1, 0.0, 0.0005], [5, 7, 5, 5, 0.1, 0.0, 0.001], [5, 5, 5, 5, 0.3, 0.0001, 0.001], [3, 7, 10, 10, 0.1, 0.0001, 0.001], [7, 3, 15, 5, 0.3, 0.0, 0.001]]\n",
    "\n",
    "# RUN10 #rand_param_combos = [[7, 5, 5, 10, 0.3, 0.0001, 0.001], [5, 5, 15, 10, 0.1, 0.0, 0.001], [3, 3, 10, 10, 0.3, 0.0001, 0.001], [5, 3, 10, 5, 0.1, 0.0001, 0.0005], [3, 7, 10, 5, 0.3, 0.0, 0.0005]]\n",
    "\n",
    "# CHANGE the file tag each time \n",
    "# each grid search run will generate an on and off .csv w/ average metrics\n",
    "# upload to github the results after each run (and cross off that the parameters have been tried on the google doc)\n",
    "# ideally name_run# where run # is the count of the search you've run \n",
    "saving_file_tag = 'jackie_additional_runs'\n",
    "\n",
    "# Also, please manually check that the seed printed out is different per run. Thanks! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. Run Grid Search. Use K-Fold CV to ensure reliability of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kfold object \n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# hidden layer width is dependent of output size of CNN layers = seq_len * num_filters\n",
    "# manually determine (fairly arbitrary)\n",
    "# dependent on number of \n",
    "hidden_layer_choices = {5: (150, 60, 15), 10: (300, 100, 30), 15: (400,150, 30),}\n",
    "\n",
    "# define parameters for training \n",
    "num_epochs = 150\n",
    "patience = int(num_epochs * .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save data as we go \n",
    "def save_per_combo(parameters, avg_metric_folds_on,avg_metric_folds_off,std_metric_folds_on,std_metric_folds_off): \n",
    "    on_df = pd.DataFrame({'Params':parameters, 'R2': avg_metric_folds_on[:,0], 'Pearson':avg_metric_folds_on[:,1], 'Spearman': avg_metric_folds_on[:,2],\n",
    "                           'R2 (std)': std_metric_folds_on[:,0], 'Pearson (std)':std_metric_folds_on[:,1], 'Spearman (std)': std_metric_folds_on[:,2],\n",
    "                         })\n",
    "\n",
    "    on_df.to_csv('grid_search_additional/on_2headed_cnn_running_reg_results.csv')\n",
    "\n",
    "    off_df =pd.DataFrame({'Params':parameters, 'R2': avg_metric_folds_off[:,0], 'Pearson':avg_metric_folds_off[:,1], 'Spearman': avg_metric_folds_off[:,2],\n",
    "                               'R2 (std)': std_metric_folds_off[:,0], 'Pearson (std)':std_metric_folds_off[:,1], 'Spearman (std)': std_metric_folds_off[:,2],\n",
    "                         })\n",
    "    off_df.to_csv('grid_search_additional/off_2headed_cnn_running_reg_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run grid search \n",
    "parameters = [str(params) for params in rand_param_combos]\n",
    "avg_metric_folds_on= []\n",
    "std_metric_folds_on= [] \n",
    "avg_metric_folds_off= []\n",
    "std_metric_folds_off= [] \n",
    "final_seeds = [] \n",
    "for param_combo_count, param_combo in enumerate(rand_param_combos): \n",
    "    \n",
    "    print('On combination #', param_combo_count)\n",
    "    # unpack the parameter combination \n",
    "    (kernel_width1, kernel_width2, num_filt1, num_filt2) = param_combo\n",
    "    dr = 0.1\n",
    "    lreg = 0.0001\n",
    "    lr = 0.0005 \n",
    "    hidden_layers = hidden_layer_choices[num_filt2] # MLP architecture dependent on filters \n",
    "    conv_layer_parameters = [(kernel_width1,num_filt1),(kernel_width2,num_filt2)]\n",
    "    print('Param combo: ', param_combo)\n",
    "    \n",
    "    cv_scores_on = []\n",
    "    cv_scores_off=[]\n",
    "    fold_count = 0\n",
    "\n",
    "    for train, test in kfold.split(X, y): \n",
    "        print('Beginning fold #', fold_count)\n",
    "        # create model\n",
    "        model = twoheaded_conv1d(conv_layer_parameters= conv_layer_parameters, hidden_layers=hidden_layers, dropout_rate=dr, reg_coeff=lreg, learning_rate=lr)\n",
    "\n",
    "        # split data again for validation set (to be used w/ early stopping)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X[test], y[test], train_size = 0.5, test_size = 0.5)\n",
    "\n",
    "        # train the model\n",
    "        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0, patience=patience, verbose=0, mode='auto')\n",
    "        model.fit(X[train], [y[train][:,0], y[train][:,1]],epochs=num_epochs, batch_size=128,verbose=2, validation_data=(X_val, [y_val[:,0], y_val[:,1]]), callbacks=[early_stopping])\n",
    "\n",
    "        # evaluate the model (for ON and OFF separately)\n",
    "        \n",
    "        def r2(preds_y, true_y):\n",
    "            return pearsonr(preds_y, true_y)[0] ** 2\n",
    "\n",
    "        def compute_metrics(preds_y, true_y): \n",
    "            r2_score = r2(preds_y, true_y)[0]\n",
    "            pearson_corr = pearsonr(preds_y, true_y)[0][0]\n",
    "            spearman_corr = spearmanr(preds_y, true_y)[0]\n",
    "            print('R2: ', r2_score)\n",
    "            print('Pearson: ', pearson_corr)\n",
    "            print('Spearman: ', spearman_corr)\n",
    "            return [r2_score, pearson_corr, spearman_corr]\n",
    "\n",
    "        y_preds = np.array(model.predict(X_test))\n",
    "        # reshape output properly\n",
    "        #predictions = np.reshape(y_preds, [np.shape(y_preds)[1], np.shape(y_preds)[0]])\n",
    "\n",
    "        # get on and off metrics separately\n",
    "        print('--- ON Metrics ---')\n",
    "        on_metrics = compute_metrics(y_preds[0],np.expand_dims(y_test[:,0], 1))\n",
    "        print('--- OFF Metrics ---')\n",
    "        off_metrics = compute_metrics(y_preds[1],np.expand_dims(y_test[:,1], 1))\n",
    "        \n",
    "        cv_scores_on.append(on_metrics)\n",
    "        cv_scores_off.append(off_metrics)\n",
    "        \n",
    "        fold_count += 1\n",
    "\n",
    "    # compute summary of metrics per parameter combo\n",
    "    avg_metric_folds_on.append(np.mean(cv_scores_on, axis = 0)) # avg over columns \n",
    "    std_metric_folds_on.append(np.std(cv_scores_on, axis = 0)) # st dev over columns\n",
    "    avg_metric_folds_off.append(np.mean(cv_scores_off, axis = 0)) # avg over columns \n",
    "    std_metric_folds_off.append(np.std(cv_scores_off, axis = 0)) # st dev over columns \n",
    "    \n",
    "    final_seeds.append(np.random.get_state()[1][0])\n",
    "    print('param seed:', np.random.get_state()[1][0])\n",
    "    \n",
    "    # save as what we have so far\n",
    "    parameters_so_far = parameters[:param_combo_count+1]\n",
    "    save_per_combo(parameters_so_far, np.array(avg_metric_folds_on),np.array(avg_metric_folds_off),np.array(std_metric_folds_on),np.array(std_metric_folds_off))\n",
    "    \n",
    "    param_combo_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. Save Metrics. \n",
    "NOTE: saved in many different files in different formats for fear of saving anything incorrectly. Bad code, but am slightly paranoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save everything individually (fear of saving incorrectly....)\n",
    "# np.savetxt('mean_metrics_2headed_cnn_reg_on.csv', avg_metric_folds_on, delimiter=\",\")\n",
    "# np.savetxt('std_metrics_2headed_cnn_reg_on.csv', std_metric_folds_on, delimiter=\",\")\n",
    "# np.savetxt('mean_metrics_2headed_cnn_reg_off.csv', avg_metric_folds_off, delimiter=\",\")\n",
    "# np.savetxt('std_metrics_2headed_cnn_reg_off.csv', std_metric_folds_off, delimiter=\",\")\n",
    "# np.savetxt('param_combos_2headed_cnn_reg.csv', parameters, fmt='%s', delimiter=\",\")\n",
    "# np.savetxt('sampling_of_seeds_reg.csv', final_seeds)#,delimeter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad code: need to convert to np array for saving later     \n",
    "avg_metric_folds_on=np.array(avg_metric_folds_on)\n",
    "avg_metric_folds_off = np.array(avg_metric_folds_off)\n",
    "std_metric_folds_on = np.array(std_metric_folds_on)\n",
    "std_metric_folds_off = np.array(std_metric_folds_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as dataframes - these are the most reliable and complete \n",
    "\n",
    "on_df = pd.DataFrame({'Params':parameters, 'R2': avg_metric_folds_on[:,0], 'Pearson':avg_metric_folds_on[:,1], 'Spearman': avg_metric_folds_on[:,2],\n",
    "                           'R2 (std)': std_metric_folds_on[:,0], 'Pearson (std)':std_metric_folds_on[:,1], 'Spearman (std)': std_metric_folds_on[:,2],\n",
    "                     })\n",
    "\n",
    "on_df.to_csv('grid_search_additional/' + saving_file_tag + 'on_2headed_cnn_reg_results.csv')\n",
    "\n",
    "off_df =pd.DataFrame({'Params':parameters, 'R2': avg_metric_folds_off[:,0], 'Pearson':avg_metric_folds_off[:,1], 'Spearman': avg_metric_folds_off[:,2],\n",
    "                           'R2 (std)': std_metric_folds_off[:,0], 'Pearson (std)':std_metric_folds_off[:,1], 'Spearman (std)': std_metric_folds_off[:,2],\n",
    "                     })\n",
    "off_df.to_csv('grid_search_additional/' + saving_file_tag + 'off_2headed_cnn_reg_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
