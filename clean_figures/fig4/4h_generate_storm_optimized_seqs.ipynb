{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the sequences to be tested experimentally. Based off the 4 'predicted bad' sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import statements \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "%matplotlib inline\n",
    "\n",
    "import keras as keras\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from pysster.One_Hot_Encoder import One_Hot_Encoder\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import isolearn.keras as iso\n",
    "from seqprop import *\n",
    "from seqprop.generator import *\n",
    "from seqprop.predictor import *\n",
    "from seqprop.optimizer import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load in sequence data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toehold length:  59\n",
      "Number of sequences:  4\n"
     ]
    }
   ],
   "source": [
    "# manually input data since there are only four data points\n",
    "toehold_seqs = ['TGTATAAACCCACAAATGTAAGTGAAAAAAAACAGAGGAGATTTTTTATGTTACATTTG', 'AATGTCCACACCCAAATTATTGAGTATTTTAACAGAGGAGAAAAATAATGAATAATTTG', 'GTTGTTTAATCCTTTAATAAAGTATAAATAAACAGAGGAGATATTTAATGTTTATTAAA', 'ATCAAAGTGTCCCTTATTTACAACATTAAAAACAGAGGAGATTTAATATGGTAAATAAG']\n",
    "storm_pred_onoff_vals = [-0.0041365, -0.0105771, -0.0318257, 0.00174482]\n",
    "seq_len = len(toehold_seqs[0])\n",
    "print('Toehold length: ', seq_len)\n",
    "num_seqs = len(toehold_seqs)\n",
    "print('Number of sequences: ', num_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Transform Data. One-hot encode sequences and extact target on and off values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (4, 59, 4)\n",
      "target shape:  (4,)\n"
     ]
    }
   ],
   "source": [
    "# create DNA alphabet- may need to change if you have RNA toeholds. Just change to 'AUCG' in the first line\n",
    "alph_letters = sorted('ACGT')\n",
    "alph = list(alph_letters)\n",
    "\n",
    "# one-hot encode with pysster (very fast and simple encoding)  \n",
    "one = One_Hot_Encoder(alph_letters)\n",
    "def _get_one_hot_encoding(seq):\n",
    "    one_hot_seq = one.encode(seq)                         \n",
    "    return one_hot_seq\n",
    "\n",
    "# now convert the data into one_hot_encoding \n",
    "input_col_name = 'switch_sequence'\n",
    "X = np.stack([_get_one_hot_encoding(s) for s in toehold_seqs]).astype(np.float32)\n",
    "print('input shape: ', X.shape)\n",
    "\n",
    "# now set y as the on and off values\n",
    "y = np.array(storm_pred_onoff_vals).astype(np.float32)\n",
    "print('target shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Load in final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = ''\n",
    "final_model_path = model_dir + 'freeze_weights_tf_onoff_model.h5'\n",
    "final_weights_path = model_dir + 'freeze_weights_tf_onoff_model_weights.h5'\n",
    "model = load_model(final_model_path)\n",
    "model.load_weights(final_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 59, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv_0 (Conv1D)              (None, 59, 10)            210       \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv1D)              (None, 59, 5)             155       \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 295)               0         \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 295)               0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 150)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                9060      \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                915       \n",
      "_________________________________________________________________\n",
      "on_output (Dense)            (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 54,756\n",
      "Trainable params: 54,391\n",
      "Non-trainable params: 365\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# visually inspect architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Build model specific for seqprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://github.com/876lkj/seqprop \n",
    "\n",
    "# need to re-create EXACT SAME layers as final trained model\n",
    "# fix weights of layers so only input layer is modified\n",
    "def load_saved_predictor(model_path) :\n",
    "\n",
    "    saved_model = load_model(model_path)\n",
    "\n",
    "    def _initialize_predictor_weights(predictor_model, saved_model=saved_model) :\n",
    "        #Load pre-trained model\n",
    "        predictor_model.get_layer('conv_0').set_weights(saved_model.get_layer('conv_0').get_weights())\n",
    "        predictor_model.get_layer('conv_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('conv_1').set_weights(saved_model.get_layer('conv_1').get_weights())\n",
    "        predictor_model.get_layer('conv_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_0').set_weights(saved_model.get_layer('dense_0').get_weights())\n",
    "        predictor_model.get_layer('dense_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_1').set_weights(saved_model.get_layer('dense_1').get_weights())\n",
    "        predictor_model.get_layer('dense_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_2').set_weights(saved_model.get_layer('dense_2').get_weights())\n",
    "        predictor_model.get_layer('dense_2').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('on_output').set_weights(saved_model.get_layer('on_output').get_weights())\n",
    "        predictor_model.get_layer('on_output').trainable = False\n",
    "\n",
    "    def _load_predictor_func(sequence_input) :\n",
    "        # input space parameters \n",
    "        seq_length = 59\n",
    "        num_letters = 4 # num nt \n",
    "        # expanded version b/c seqprop built for 2d \n",
    "        seq_input_shape = (seq_len, num_letters, 1) # modified\n",
    "\n",
    "        #define new model definition (same architecture except modified input)\n",
    "        dropout_rate=0.1\n",
    "        reg_coeff= 0.0001\n",
    "        hidden_layer_choices = {5: (150, 60, 15),}\n",
    "        conv_layer_parameters = [(5,10), (3,5),]\n",
    "        hidden_layers = hidden_layer_choices[5]\n",
    "        \n",
    "        reshaped_input = Reshape(target_shape=(seq_len, num_letters),name='reshaped_input')(sequence_input)\n",
    "        prior_layer = reshaped_input \n",
    "        for idx, (kernel_width, num_filters) in enumerate(conv_layer_parameters):\n",
    "            conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_width, padding='same', name='conv_'+str(idx))(prior_layer) # mimic a kmer\n",
    "            prior_layer = conv_layer\n",
    "        H = Flatten(name='flatten')(prior_layer)\n",
    "        for idx,h in enumerate(hidden_layers): \n",
    "            H = Dropout(dropout_rate, name='dropout_'+str(idx))(H)\n",
    "            H = Dense(h, activation='relu', kernel_regularizer=l2(reg_coeff), name='dense_'+str(idx))(H)\n",
    "        out_onoff = Dense(1,activation=\"linear\",name='on_output')(H)\n",
    "        \n",
    "        predictor_inputs = []\n",
    "        predictor_outputs = [out_onoff]\n",
    "\n",
    "        return predictor_inputs, predictor_outputs, _initialize_predictor_weights\n",
    "\n",
    "    return _load_predictor_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. Set-up gradient ascent workflow. Convert to callable function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants \n",
    "\n",
    "# get seed input which we will modify \n",
    "num_samples = 1\n",
    "\n",
    "# template specifying what to modify and what not (biological constaints)\n",
    "switch = 'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
    "rbs = 'AACAGAGGAGA'\n",
    "start_codon = 'ATG'\n",
    "stem1 = 'NNNNNN'#'XXXXXX'\n",
    "stem2 = 'NNNNNNNNN'#'XXXXXXXXX'\n",
    "\n",
    "bio_constraints = switch + rbs + stem1 + start_codon + stem2 \n",
    "\n",
    "# define target on/off values \n",
    "target_onoff = 1\n",
    "target = [[target_onoff], ] # keep in this format in case you want to adapt for separate on and off predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loss function\n",
    "# ensure biological constraints are satisfied per sequence\n",
    "\n",
    "def stem_base_pairing(pwm): \n",
    "    # ensure that location of 1s in switch region matches reverse complement of stem\n",
    "    \n",
    "    def reverse_complement(base_index): \n",
    "        # ACGT = alphabett\n",
    "        if base_index == 0: return 3\n",
    "        elif base_index == 1: return 2 \n",
    "        elif base_index == 2: return 1 \n",
    "        elif base_index == 3: return 0\n",
    "    \n",
    "    # reverse complement is reverse over axis of one-hot encoded nt \n",
    "    nt_reversed = K.reverse(pwm, axes=2)\n",
    "    stem1_score = 6 - K.sum(pwm[:, 24, :, 0]*nt_reversed[:, 41,:, 0] + pwm[:, 25, :, 0]*nt_reversed[:, 42, :, 0]+ pwm[:,26, :, 0]*nt_reversed[:, 43, :, 0] + pwm[:, 27, :, 0]*nt_reversed[:, 44, :, 0] + pwm[:, 28, :, 0]*nt_reversed[:, 45, :, 0]+ pwm[:, 29, :, 0]*nt_reversed[:, 46, :, 0])\n",
    "    stem2_score = 9 - K.sum(pwm[:, 12, :, 0]*nt_reversed[:, 50, :, 0] + pwm[:, 13, :, 0]*nt_reversed[:, 51, :, 0]+ pwm[:, 14, :, 0]*nt_reversed[:, 52, :, 0]+ pwm[:, 15, :, 0]*nt_reversed[:, 53, :, 0] + pwm[:, 16, :, 0]*nt_reversed[:, 54, :, 0] + pwm[:, 17, :, 0]*nt_reversed[:,55, :, 0]+ pwm[:, 18,:, 0]*nt_reversed[:, 56, :, 0] + pwm[:, 19, :, 0]*nt_reversed[:,57, :, 0] + pwm[:, 20, :, 0]*nt_reversed[:, 58, :, 0])\n",
    "    return 10*stem1_score + 10*stem2_score\n",
    "\n",
    "def loss_func(predictor_outputs) :\n",
    "    pwm_logits, pwm, sampled_pwm, predicted_out = predictor_outputs\n",
    "  \n",
    "    #Create target constant -- want predicted value for modified input to be close to target input \n",
    "    target_out = K.tile(K.constant(target), (K.shape(sampled_pwm)[0], 1))\n",
    "    target_cost = (target_out - predicted_out)**2\n",
    "    print(target_out, target_cost, predicted_out)\n",
    "    base_pairing_cost = stem_base_pairing(sampled_pwm)\n",
    "    print(base_pairing_cost)\n",
    "    print(K.mean(target_cost + base_pairing_cost, axis=-1))\n",
    "    \n",
    "    ## use this return statement to include the basepairing cost\n",
    "    #return K.mean(target_cost + base_pairing_cost, axis=-1)\n",
    "    \n",
    "    ## use this return statement to ignore the basepairing cost\n",
    "    # modifying so we don't have the base pairing constraint- will make sure complementary after\n",
    "    return K.mean(target_cost, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_ascent(input_toehold_seq, original_out):\n",
    "\n",
    "    # build generator network\n",
    "    _, seqprop_generator = build_generator(seq_length=seq_len, n_sequences=num_samples, batch_normalize_pwm=True,init_sequences = [input_toehold_seq],\n",
    "                                          sequence_templates=bio_constraints)# batch_normalize_pwm=True)\n",
    "    \n",
    "    # build predictor network and hook it on the generator PWM output tensor\n",
    "    _, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(final_model_path), n_sequences=num_samples, eval_mode='pwm')\n",
    "\n",
    "    #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "    _, loss_model = build_loss_model(seqprop_predictor, loss_func, )\n",
    "\n",
    "    #Specify Optimizer to use\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    #Compile Loss Model (Minimize self)\n",
    "    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "    #Fit Loss Model\n",
    "    #seed_input = np.reshape([X[0]], [1,59,4,1]) # any input toehold to be modified\n",
    "\n",
    "    callbacks =[\n",
    "                EarlyStopping(monitor='loss', min_delta=0.001, patience=5, verbose=0, mode='auto'),\n",
    "                #SeqPropMonitor(predictor=seqprop_predictor)#, plot_every_epoch=True, track_every_step=True, )#cse_start_pos=70, isoform_start=target_cut, isoform_end=target_cut+1, pwm_start=70-40, pwm_end=76+50, sequence_template=sequence_template, plot_pwm_indices=[0])\n",
    "            ]\n",
    "\n",
    "    num_epochs=50\n",
    "    train_history = loss_model.fit([], np.ones((1, 1)), epochs=num_epochs, steps_per_epoch=1000, callbacks=callbacks)\n",
    "\n",
    "    #Retrieve optimized PWMs and predicted (optimized) target\n",
    "    _, optimized_pwm, optimized_onehot, predicted_out = seqprop_predictor.predict(x=None, steps=1)\n",
    "    print('Original ON/OFF:', original_out)\n",
    "    print('Predicted ON/OFF: ', predicted_out)\n",
    "    \n",
    "    return optimized_pwm, optimized_onehot, predicted_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7. Run gradient ascent on the specified seed inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_onehot(oh_seq): \n",
    "    return ''.join(alph[idx] for idx in np.argmax(oh_seq,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_5/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_5/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_12/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_5/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_5/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4656\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3230\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2447\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1930\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1718\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1647\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1634\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1645\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1642\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1649\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1629\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1634\n",
      "Original ON/OFF: -0.0041365\n",
      "Predicted ON/OFF:  [[0.61054814]]\n",
      "Tensor(\"lambda_6/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_6/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_14/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_6/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_6/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3700\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1187\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0337\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0239\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0186\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0172\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0181\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0167\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0166\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0171\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0166\n",
      "Original ON/OFF: -0.0105771\n",
      "Predicted ON/OFF:  [[0.9805699]]\n",
      "Tensor(\"lambda_7/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_7/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_16/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_7/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_7/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4147\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2510\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1794\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1641\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1623\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1501\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1418\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1392\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1346\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1309\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1270\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1268\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1221\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1110\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0994\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0498\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0199\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0171\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0169\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0177\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0180\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0158\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0171\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0164\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0162\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0179\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0165\n",
      "Original ON/OFF: -0.0318257\n",
      "Predicted ON/OFF:  [[0.9823758]]\n",
      "Tensor(\"lambda_8/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_8/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_18/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_8/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_8/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4559\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2204\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0229\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0179\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0175\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0185\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0164\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0163\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0173\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0157\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0161\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0160\n",
      "Original ON/OFF: 0.00174482\n",
      "Predicted ON/OFF:  [[0.9809136]]\n",
      "Tensor(\"lambda_9/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_9/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_20/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_9/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_9/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4646\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3206\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2413\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1928\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1688\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1650\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1639\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1628\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1637\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1635\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1646\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1633\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1624\n",
      "Original ON/OFF: -0.0041365\n",
      "Predicted ON/OFF:  [[0.61039]]\n",
      "Tensor(\"lambda_10/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_10/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_22/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_10/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_10/Mean:0\", shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3792\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1282\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0367\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0265\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0208\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0185\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0173\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0183\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0172\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0162\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0168\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0166\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0164\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0172\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0163\n",
      "Original ON/OFF: -0.0105771\n",
      "Predicted ON/OFF:  [[0.97903997]]\n",
      "Tensor(\"lambda_11/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_11/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_24/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_11/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_11/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4078\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2571\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1835\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1638\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1577\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1486\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1437\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1382\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1340\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1284\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1266\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1267\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1249\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1230\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1231\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1222\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1230\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1231\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1218\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1237\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1224\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1209\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1225\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1219\n",
      "Original ON/OFF: -0.0318257\n",
      "Predicted ON/OFF:  [[0.66565937]]\n",
      "Tensor(\"lambda_12/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_12/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_26/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_12/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_12/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4596\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2321\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0243\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0180\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0171\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0163\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0174\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0163\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0166\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0159\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0160\n",
      "Original ON/OFF: 0.00174482\n",
      "Predicted ON/OFF:  [[0.9762535]]\n",
      "Tensor(\"lambda_13/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_13/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_28/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_13/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_13/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4576\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3125\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2412\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1926\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1683\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1654\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1646\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1622\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1630\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1630\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1622\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1635\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1621\n",
      "Original ON/OFF: -0.0041365\n",
      "Predicted ON/OFF:  [[0.61037016]]\n",
      "Tensor(\"lambda_14/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_14/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_30/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_14/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_14/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3713\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1242\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0357\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0238\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0197\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0176\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0178\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0172\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0158\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0174\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0160\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0163\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0164\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0167\n",
      "Original ON/OFF: -0.0105771\n",
      "Predicted ON/OFF:  [[0.9836762]]\n",
      "Tensor(\"lambda_15/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_15/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_32/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_15/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_15/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4080\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2559\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1837\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1651\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1617\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1491\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1400\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1377\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1361\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1336\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1341\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1335\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1310\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1359\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1328\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1316\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1323\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1333\n",
      "Original ON/OFF: -0.0318257\n",
      "Predicted ON/OFF:  [[0.64880574]]\n",
      "Tensor(\"lambda_16/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_16/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_34/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_16/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_16/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4633\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2792\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0307\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0173\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0162\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0170\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0167\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0167\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0157\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0168\n",
      "Original ON/OFF: 0.00174482\n",
      "Predicted ON/OFF:  [[0.9767877]]\n",
      "Tensor(\"lambda_17/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_17/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_36/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_17/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_17/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4614\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3179\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2442\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1927\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1690\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1682\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1649\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1634\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1654\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1657\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1645\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1651\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1649\n",
      "Original ON/OFF: -0.0041365\n",
      "Predicted ON/OFF:  [[0.6098346]]\n",
      "Tensor(\"lambda_18/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_18/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_38/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_18/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_18/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3780\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1253\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0354\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0248\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0190\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0191\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0176\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0172\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0164\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0172\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0165\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0168\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0175\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0171\n",
      "Original ON/OFF: -0.0105771\n",
      "Predicted ON/OFF:  [[0.97487193]]\n",
      "Tensor(\"lambda_19/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_19/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_40/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_19/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_19/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4116\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2432\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1753\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1655\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1521\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1443\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1398\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1385\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1312\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1277\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1287\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1258\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1223\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1083\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0917\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0401\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0188\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0184\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0178\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0160\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0166\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0170\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0161\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0163\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0170\n",
      "Original ON/OFF: -0.0318257\n",
      "Predicted ON/OFF:  [[0.9792814]]\n",
      "Tensor(\"lambda_20/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_20/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_42/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_20/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_20/Mean:0\", shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4602\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2506\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0255\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0186\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0178\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0184\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0168\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0162\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0176\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0175\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0162\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0154\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0163\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0172\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0154\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0154\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0161\n",
      "Original ON/OFF: 0.00174482\n",
      "Predicted ON/OFF:  [[0.9804386]]\n",
      "Tensor(\"lambda_21/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_21/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_44/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_21/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_21/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.4646\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3203\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2440\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1915\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1674\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1640\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1639\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1630\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1647\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1627\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1637\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1628\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1661\n",
      "Original ON/OFF: -0.0041365\n",
      "Predicted ON/OFF:  [[0.6102339]]\n",
      "Tensor(\"lambda_22/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_22/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_46/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_22/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_22/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3754\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1243\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0335\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0253\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0195\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0176\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0184\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0172\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0176\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0172\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0158\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0170\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0167\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0155\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0167\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0165\n",
      "Original ON/OFF: -0.0105771\n",
      "Predicted ON/OFF:  [[0.97565174]]\n",
      "Tensor(\"lambda_23/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_23/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_48/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_23/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_23/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.4123\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2462\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1824\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1630\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1631\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1629\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1601\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1615\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1527\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1450\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1407\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1325\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1332\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1266\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1271\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1243\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1234\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1196\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1224\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1216\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1214\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1219\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1224\n",
      "Original ON/OFF: -0.0318257\n",
      "Predicted ON/OFF:  [[0.6677283]]\n",
      "Tensor(\"lambda_24/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_24/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_50/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_24/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_24/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.4648\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2830\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0323\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0182\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0177\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0177\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0173\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0163\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0167\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0163\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0160\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0163\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0166\n",
      "Original ON/OFF: 0.00174482\n",
      "Predicted ON/OFF:  [[0.97485113]]\n"
     ]
    }
   ],
   "source": [
    "optimized_pwms = [] # store the probabilities\n",
    "optimized_seqs = [] # store the converted sequences to be tested \n",
    "predicted_targets = [] # store the original and predicted target values \n",
    "\n",
    "# run 5 optimization rounds for each sequence- part of STORM algorithm\n",
    "num_of_optimization_rounds = 5\n",
    "for i in range(0, num_of_optimization_rounds):\n",
    "    for idx, (toehold_seq, original_out) in enumerate(zip(toehold_seqs, y)): \n",
    "        optimized_pwm, optimized_onehot, predicted_out = run_gradient_ascent(toehold_seq, original_out)\n",
    "        optimized_pwms.append(np.reshape(optimized_pwm, [59, 4]))\n",
    "        predicted_targets.append(predicted_out)\n",
    "        new_seq = invert_onehot(np.reshape(optimized_onehot, [59,4]))\n",
    "        optimized_seqs.append(new_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8. Change toeholds to adhere to basepairing and toehold structure- post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame()\n",
    "data_df['old_switches'] = toehold_seqs*num_of_optimization_rounds\n",
    "data_df['old_onoff'] = storm_pred_onoff_vals*num_of_optimization_rounds\n",
    "data_df['new_switch'] = optimized_seqs\n",
    "data_df['predicted_onoff'] = predicted_targets\n",
    "data_df['optimized_pwm'] = optimized_pwms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbs = 'AACAGAGGAGA'\n",
    "start_codon = 'ATG'\n",
    "\n",
    "# Make function to generate reverse compliment of the DNA strand\n",
    "def make_rev_complement(string):\n",
    "    new_str = ''\n",
    "    for s in string:\n",
    "        char = ''\n",
    "        if s == 'A':\n",
    "            char = 'T'\n",
    "        elif s == 'T':\n",
    "            char = 'A'\n",
    "        elif s == 'C':\n",
    "            char = 'G'\n",
    "        elif s == 'G':\n",
    "            char = 'C'\n",
    "        else:\n",
    "            print('UH OH! Character not A, T, C, or G')\n",
    "        new_str += char\n",
    "    new_str = new_str[::-1]\n",
    "    return new_str\n",
    "\n",
    "# Make function to check for stop codons\n",
    "def check_for_stop(toehold): \n",
    "    stop_codons = ['TAG', 'TAA', 'TGA']\n",
    "    location_of_start = 47\n",
    "    search1 = toehold.find(stop_codons[0]) == location_of_start\n",
    "    search2 = toehold.find(stop_codons[1]) == location_of_start\n",
    "    search3 = toehold.find(stop_codons[2]) == location_of_start\n",
    "    return (search1 | search2  | search3)\n",
    "\n",
    "# Make function to actually turn trigger into toehold\n",
    "def turn_switch_to_toehold(switch):\n",
    "    stem1 = make_rev_complement(switch[24:30])\n",
    "    stem2 = make_rev_complement(switch[12:21])\n",
    "    toehold = switch + rbs + stem1 + start_codon + stem2\n",
    "    return toehold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rev comp\n",
    "def check_rev_comp(full_59nt):\n",
    "    stem1 = make_rev_complement(full_59nt[24:30])\n",
    "    stem2 = make_rev_complement(full_59nt[12:21])\n",
    "    stem1_comp = full_59nt[41:47]\n",
    "    stem2_comp = full_59nt[50:59]\n",
    "    \n",
    "    return((stem1 == stem1_comp) and (stem2 == stem2_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rbs and start codon are unchanged\n",
    "def check_rbs_and_start(full_59nt):\n",
    "    rbs_exists = (full_59nt[30:41] == rbs)\n",
    "    start_exists = (full_59nt[47:50] == start_codon)\n",
    "    return(rbs_exists and start_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TCCTTCGGCATCTACATCTATATAAAACGAAACAGAGGAGATCGTTTATGATAGATGTA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCTGCTCTTCCCCTCCAAACAGAGGAGATGGAGGATGAGAGCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CGCAATATTATCTGCTGGCCTACCCCTCCAAACAGAGGAGATGGAGGATGAGGCCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCTGCTCCTCCCCTCCAAACAGAGGAGATGGAGGATGGGAGCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TCCTTCGGCATCTACCTCTATATAAAACGAAACAGAGGAGATCGTTTATGATAGAGGTA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCTGGTCTTCCCCTCCAAACAGAGGAGATGGAGGATGAGACCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "GATGGTAAAAACCCGACAACTATATATCTAAACAGAGGAGATAGATAATGAGTTGTCGG\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCTGCTCTTCCCCTCCAAACAGAGGAGATGGAGGATGAGAGCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TCCTTCGGTATCTACCTCTATATAAAACGAAACAGAGGAGATCGTTTATGATAGAGGTA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CTCATTATTATCTGCTGGTCTACCCCTCCAAACAGAGGAGATGGAGGATGAGACCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CATGGTAACAACCCGACAAATAGATACATAAACAGAGGAGATATGTAATGATTTGTCGG\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCTGCTCCTCCCCTCCAAACAGAGGAGATGGAGGATGGGAGCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TCCTTCGGTATCTACCTCTATGTAAAACGAAACAGAGGAGATCGTTTATGATAGAGGTA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCTGGTCTACCCCTCCAAACAGAGGAGATGGAGGATGAGACCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CTAAATATTATCTGCGGGCCTTCGCCTCCAAACAGAGGAGATGGAGGATGAGGCCCGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCTGCTCCTCCCCTCCAAACAGAGGAGATGGAGGATGGGAGCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TCCTTCGGCATCTACCTCTATATAAAACGAAACAGAGGAGATCGTTTATGATAGAGGTA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CTCATTATTATCTGCTGGTCTTCCCCTCCAAACAGAGGAGATGGAGGATGAGACCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "GATGGTTAAAACCCGACAACTATAGATCTAAACAGAGGAGATAGATCATGAGTTGTCGG\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCTGCTCTTCCCCTCCAAACAGAGGAGATGGAGGATGAGAGCAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n"
     ]
    }
   ],
   "source": [
    "# convert new switches to bp complementarity / toehold structure\n",
    "new_fixed_switches = []\n",
    "for toehold in data_df['new_switch']:\n",
    "    base_30nt = toehold[0:30]\n",
    "    print('checking for rev comp: ', check_rev_comp(toehold))\n",
    "    print('checking for rbs and start codon: ', check_rbs_and_start(toehold))\n",
    "    new_toehold = turn_switch_to_toehold(base_30nt)\n",
    "    print(new_toehold)\n",
    "    print('checking for rev comp: ', check_rev_comp(new_toehold))\n",
    "    print('checking for rbs and start codon: ', check_rbs_and_start(new_toehold))\n",
    "    new_fixed_switches.append(new_toehold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['NEW_fixed_switch'] = new_fixed_switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5579137 ]\n",
      " [0.54170436]\n",
      " [0.3156554 ]\n",
      " [0.38759553]\n",
      " [0.51075286]\n",
      " [0.5078366 ]\n",
      " [0.5785158 ]\n",
      " [0.54170436]\n",
      " [0.5231471 ]\n",
      " [0.4552125 ]\n",
      " [0.57521087]\n",
      " [0.38759553]\n",
      " [0.5308206 ]\n",
      " [0.4570665 ]\n",
      " [0.1490827 ]\n",
      " [0.38759553]\n",
      " [0.51075286]\n",
      " [0.50698686]\n",
      " [0.61387956]\n",
      " [0.54170436]]\n"
     ]
    }
   ],
   "source": [
    "X = np.stack([_get_one_hot_encoding(s) for s in new_fixed_switches]).astype(np.float32)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "#print(predictions)\n",
    "\n",
    "data_df['NEW_onoff_preds'] = np.reshape(predictions, [num_seqs*num_of_optimization_rounds,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'data/'\n",
    "data_df.to_csv(out_dir + '4h_final_test_storm_optimized.csv') # select top sequence from each round of 5 afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_toehold_venv",
   "language": "python",
   "name": "clean_toehold_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
