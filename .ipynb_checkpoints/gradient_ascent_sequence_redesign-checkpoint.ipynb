{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Optimize the toehold sequences with gradient ascent to improve the ON/OFF ratio.\n",
    "\n",
    "### Instructions: Please change the file_name in the second code block to sequences you are interested in redesigning. The format should be a .csv file with at least three columns: a switch_sequence column with the original DNA sequence of the toehold; an on_value column with the ON value of the switch (can be predicted if in silico); and an off_value column with the OFF value of the switch (again, can be predicted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import statements \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "%matplotlib inline\n",
    "\n",
    "import keras as keras\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from pysster.One_Hot_Encoder import One_Hot_Encoder\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import isolearn.keras as iso\n",
    "from seqprop import *\n",
    "import seqprop.visualization\n",
    "from seqprop.generator import *\n",
    "from seqprop.predictor import *\n",
    "from seqprop.optimizer import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load in sequence data. \n",
    "## Change file_name here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>switch_sequence</th>\n",
       "      <th>on_value</th>\n",
       "      <th>off_value</th>\n",
       "      <th>onoff_value</th>\n",
       "      <th>on_preds</th>\n",
       "      <th>off_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACAAAAAAACAATAAAAAATAGAGAAAAAGAACAGAGGAGACTTTT...</td>\n",
       "      <td>0.428270</td>\n",
       "      <td>0.818291</td>\n",
       "      <td>-0.390021</td>\n",
       "      <td>0.521815</td>\n",
       "      <td>0.815901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATAAACAAAATGGATATTATAGACAAAAAAAACAGAGGAGATTTTT...</td>\n",
       "      <td>0.570486</td>\n",
       "      <td>0.934635</td>\n",
       "      <td>-0.364150</td>\n",
       "      <td>0.700090</td>\n",
       "      <td>0.864703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GATGTTACAAACGATAATATAGACAAAAATAACAGAGGAGAATTTT...</td>\n",
       "      <td>0.642210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.357790</td>\n",
       "      <td>0.718297</td>\n",
       "      <td>0.850942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     switch_sequence  on_value  off_value  \\\n",
       "0  ACAAAAAAACAATAAAAAATAGAGAAAAAGAACAGAGGAGACTTTT...  0.428270   0.818291   \n",
       "1  ATAAACAAAATGGATATTATAGACAAAAAAAACAGAGGAGATTTTT...  0.570486   0.934635   \n",
       "2  GATGTTACAAACGATAATATAGACAAAAATAACAGAGGAGAATTTT...  0.642210   1.000000   \n",
       "\n",
       "   onoff_value  on_preds  off_preds  \n",
       "0    -0.390021  0.521815   0.815901  \n",
       "1    -0.364150  0.700090   0.864703  \n",
       "2    -0.357790  0.718297   0.850942  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter a .csv with sequences\n",
    "data_dir = 'gradient_ascent_sequences/'\n",
    "file_name = 'worst_toehold_sequences.csv' # CHANGE FILENAME!\n",
    "data_df = pd.read_csv(data_dir + file_name,sep=',')\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toehold length:  59\n",
      "Number of sequences:  100\n"
     ]
    }
   ],
   "source": [
    "toehold_seqs = data_df['switch_sequence']\n",
    "seq_len = len(toehold_seqs[0])\n",
    "print('Toehold length: ', seq_len)\n",
    "num_seqs = len(data_df)\n",
    "print('Number of sequences: ', num_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Extract toeholds to optimize.\n",
    "### Note: 100 sequences takes ~2 hours to optimize, given compute power, so simplify to just 10 sequences here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences:  3\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df[0:10]\n",
    "toehold_seqs = data_df['switch_sequence']\n",
    "print('Number of sequences: ', len(toehold_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Transform Data. One-hot encode sequences and extact target on and off values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (3, 59, 4)\n",
      "target shape:  (3, 2)\n"
     ]
    }
   ],
   "source": [
    "# create DNA alphabet- may need to change if you have RNA toeholds. Just change to 'AUCG' in the first line\n",
    "alph_letters = sorted('ATCG')\n",
    "alph = list(alph_letters)\n",
    "\n",
    "# one-hot encode with pysster (very fast and simple encoding)  \n",
    "one = One_Hot_Encoder(alph_letters)\n",
    "def _get_one_hot_encoding(seq):\n",
    "    one_hot_seq = one.encode(seq)                         \n",
    "    return one_hot_seq\n",
    "\n",
    "# now convert the data into one_hot_encoding \n",
    "input_col_name = 'switch_sequence'\n",
    "X = np.stack([_get_one_hot_encoding(s) for s in toehold_seqs]).astype(np.float32)\n",
    "print('input shape: ', X.shape)\n",
    "\n",
    "# now set y as the on and off values\n",
    "y_on = np.array(data_df['on_value'].astype(np.float32))\n",
    "y_off = np.array(data_df['off_value'].astype(np.float32))\n",
    "y = np.transpose(np.array([y_on,y_off,]))\n",
    "print('target shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Load in final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'trained_model/'\n",
    "final_model_path = model_dir + 'final_trained_model.h5'\n",
    "final_weights_path = model_dir + 'final_trained_model_weights.h5'\n",
    "model = load_model(final_model_path)\n",
    "model.load_weights(final_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 59, 4)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv1D)                 (None, 59, 10)       210         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv1D)                 (None, 59, 5)        155         conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 295)          0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 295)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 150)          44400       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 150)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 60)           9060        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 60)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15)           915         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "on_output (Dense)               (None, 1)            16          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "off_output (Dense)              (None, 1)            16          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 54,772\n",
      "Trainable params: 54,772\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# visually inspect architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. Build model specific for seqprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://github.com/876lkj/seqprop \n",
    "\n",
    "# need to re-create EXACT SAME layers as final trained model\n",
    "# fix weights of layers so only input layer is modified\n",
    "def load_saved_predictor(model_path) :\n",
    "\n",
    "    saved_model = load_model(model_path)\n",
    "\n",
    "    def _initialize_predictor_weights(predictor_model, saved_model=saved_model) :\n",
    "        #Load pre-trained model\n",
    "    \n",
    "        predictor_model.get_layer('conv_0').set_weights(saved_model.get_layer('conv_0').get_weights())\n",
    "        predictor_model.get_layer('conv_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('conv_1').set_weights(saved_model.get_layer('conv_1').get_weights())\n",
    "        predictor_model.get_layer('conv_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_0').set_weights(saved_model.get_layer('dense_0').get_weights())\n",
    "        predictor_model.get_layer('dense_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_1').set_weights(saved_model.get_layer('dense_1').get_weights())\n",
    "        predictor_model.get_layer('dense_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_2').set_weights(saved_model.get_layer('dense_2').get_weights())\n",
    "        predictor_model.get_layer('dense_2').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('on_output').set_weights(saved_model.get_layer('on_output').get_weights())\n",
    "        predictor_model.get_layer('on_output').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('off_output').set_weights(saved_model.get_layer('off_output').get_weights())\n",
    "        predictor_model.get_layer('off_output').trainable = False\n",
    "\n",
    "    def _load_predictor_func(sequence_input) :\n",
    "        # input space parameters \n",
    "        seq_length = 59\n",
    "        num_letters = 4 # num nt \n",
    "        # expanded version b/c seqprop built for 2d \n",
    "        seq_input_shape = (seq_len, num_letters, 1) # modified\n",
    "\n",
    "        #define new model definition (same architecture except modified input)\n",
    "        dropout_rate=0.1\n",
    "        reg_coeff= 0.0001\n",
    "        hidden_layer_choices = {5: (150, 60, 15), 10: (300, 100, 30), 15: (400,150, 30),}\n",
    "        conv_layer_parameters = [(5,10), (3,5),]\n",
    "        hidden_layers = hidden_layer_choices[5]\n",
    "        \n",
    "        #expanded_input = Input(shape=seq_input_shape,name='new_input')\n",
    "        reshaped_input = Reshape(target_shape=(seq_len, num_letters),name='reshaped_input')(sequence_input)#(expanded_input)        #(kernel_width, num_filters) = conv_layer_parameters\n",
    "        prior_layer = reshaped_input \n",
    "        for idx, (kernel_width, num_filters) in enumerate(conv_layer_parameters):\n",
    "            conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_width, padding='same', name='conv_'+str(idx))(prior_layer) # mimic a kmer\n",
    "            prior_layer = conv_layer\n",
    "        H = Flatten(name='flatten')(prior_layer)\n",
    "        for idx,h in enumerate(hidden_layers): \n",
    "            H = Dropout(dropout_rate, name='dropout_'+str(idx))(H)\n",
    "            H = Dense(h, activation='relu', kernel_regularizer=l2(reg_coeff), name='dense_'+str(idx))(H)\n",
    "        out_on = Dense(1,activation=\"linear\",name='on_output')(H)\n",
    "        out_off = Dense(1, activation='linear', name='off_output')(H)\n",
    "        on_off_out = Concatenate(name='on_of_output')([out_on,out_off])\n",
    "        \n",
    "        predictor_inputs = []\n",
    "        predictor_outputs = [on_off_out]\n",
    "\n",
    "        return predictor_inputs, predictor_outputs, _initialize_predictor_weights\n",
    "\n",
    "    return _load_predictor_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. Set-up gradient ascent workflow. Convert to callable function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants \n",
    "\n",
    "# get seed input which we will modify \n",
    "num_samples = 1\n",
    "\n",
    "# template specifying what to modify and what not (biological constaints)\n",
    "switch = 'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
    "rbs = 'AACAGAGGAGA'\n",
    "start_codon = 'ATG'\n",
    "stem1 = 'NNNNNN'#'XXXXXX'\n",
    "stem2 = 'NNNNNNNNN'#'XXXXXXXXX'\n",
    "\n",
    "bio_constraints = switch + rbs + stem1 + start_codon + stem2 \n",
    "\n",
    "# define target on, off values \n",
    "target_on = 0.99\n",
    "target_off = 0.001\n",
    "target = [[target_on,target_off], ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loss function\n",
    "# ensure biological constraints are satisfied per sequence\n",
    "\n",
    "def stem_base_pairing(pwm): \n",
    "    # ensure that location of 1s in switch region matches reverse complement of stem\n",
    "    \n",
    "    def reverse_complement(base_index): \n",
    "        # ACGT = alphabett\n",
    "        if base_index == 0: return 3\n",
    "        elif base_index == 1: return 2 \n",
    "        elif base_index == 2: return 1 \n",
    "        elif base_index == 3: return 0\n",
    "    \n",
    "    # reverse complement is reverse over axis of one-hot encoded nt \n",
    "    nt_reversed = K.reverse(pwm, axes=2)\n",
    "    stem1_score = 6 - K.sum(pwm[:, 24, :, 0]*nt_reversed[:, 41,:, 0] + pwm[:, 25, :, 0]*nt_reversed[:, 42, :, 0]+ pwm[:,26, :, 0]*nt_reversed[:, 43, :, 0] + pwm[:, 27, :, 0]*nt_reversed[:, 44, :, 0] + pwm[:, 28, :, 0]*nt_reversed[:, 45, :, 0]+ pwm[:, 29, :, 0]*nt_reversed[:, 46, :, 0])\n",
    "    stem2_score = 9 - K.sum(pwm[:, 12, :, 0]*nt_reversed[:, 50, :, 0] + pwm[:, 13, :, 0]*nt_reversed[:, 51, :, 0]+ pwm[:, 14, :, 0]*nt_reversed[:, 52, :, 0]+ pwm[:, 15, :, 0]*nt_reversed[:, 53, :, 0] + pwm[:, 16, :, 0]*nt_reversed[:, 54, :, 0] + pwm[:, 17, :, 0]*nt_reversed[:,55, :, 0]+ pwm[:, 18,:, 0]*nt_reversed[:, 56, :, 0] + pwm[:, 19, :, 0]*nt_reversed[:,57, :, 0] + pwm[:, 20, :, 0]*nt_reversed[:, 58, :, 0])\n",
    "    return 10*stem1_score + 10*stem2_score\n",
    "\n",
    "def loss_func(predictor_outputs) :\n",
    "    pwm_logits, pwm, sampled_pwm, predicted_out = predictor_outputs\n",
    "  \n",
    "    #Create target constant -- want predicted value for modified input to be close to target input \n",
    "    target_out = K.tile(K.constant(target), (K.shape(sampled_pwm)[0], 1))\n",
    "    target_cost = (target_out - predicted_out)**2\n",
    "    print(target_out, target_cost, predicted_out)\n",
    "    base_pairing_cost = stem_base_pairing(sampled_pwm)\n",
    "    print(base_pairing_cost)\n",
    "    print(K.mean(target_cost + base_pairing_cost, axis=-1))\n",
    "    return K.mean(target_cost + base_pairing_cost, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_ascent(input_toehold_seq, original_out):\n",
    "\n",
    "    # build generator network\n",
    "    _, seqprop_generator = build_generator(seq_length=seq_len, n_sequences=num_samples, batch_normalize_pwm=True,init_sequences = [input_toehold_seq],\n",
    "                                          sequence_templates=bio_constraints)# batch_normalize_pwm=True)\n",
    "    \n",
    "    # build predictor network and hook it on the generator PWM output tensor\n",
    "    _, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(final_model_path), n_sequences=num_samples, eval_mode='pwm')\n",
    "\n",
    "    #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "    _, loss_model = build_loss_model(seqprop_predictor, loss_func, )\n",
    "\n",
    "    #Specify Optimizer to use\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    #Compile Loss Model (Minimize self)\n",
    "    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "    #Fit Loss Model\n",
    "    #seed_input = np.reshape([X[0]], [1,59,4,1]) # any input toehold to be modified\n",
    "\n",
    "    callbacks =[\n",
    "                EarlyStopping(monitor='loss', min_delta=0.001, patience=5, verbose=0, mode='auto'),\n",
    "                #SeqPropMonitor(predictor=seqprop_predictor)#, plot_every_epoch=True, track_every_step=True, )#cse_start_pos=70, isoform_start=target_cut, isoform_end=target_cut+1, pwm_start=70-40, pwm_end=76+50, sequence_template=sequence_template, plot_pwm_indices=[0])\n",
    "            ]\n",
    "\n",
    "\n",
    "    num_epochs=50\n",
    "    train_history = loss_model.fit([], np.ones((1, 1)), epochs=num_epochs, steps_per_epoch=1000, callbacks=callbacks)\n",
    "\n",
    "    #Retrieve optimized PWMs and predicted (optimized) target\n",
    "    _, optimized_pwm, optimized_onehot, predicted_out = seqprop_predictor.predict(x=None, steps=1)\n",
    "    print('Original [on, off]:', original_out)\n",
    "    print('Predicted [on, off]: ', predicted_out)\n",
    "    \n",
    "    return optimized_pwm, optimized_onehot, predicted_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7. Run gradient ascent on the specified seed inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_onehot(oh_seq): \n",
    "    return ''.join(alph[idx] for idx in np.argmax(oh_seq,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/seqprop-0.1-py3.7.egg/seqprop/generator/seqprop_generator.py:26: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "Tensor(\"lambda_1/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_1/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_1/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_1/Mean:0\", shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 51.2187\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 9.3696\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1828\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7659\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5173\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3859\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2733\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2471\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2259\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2251\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2149\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2219\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2091\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2108\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2083\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2062\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2116\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2084\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2111\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2073\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2122\n",
      "Original [on, off]: [0.42827037 0.81829125]\n",
      "Predicted [on, off]:  [[0.4958017 0.2758617]]\n",
      "Tensor(\"lambda_2/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_2/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_1/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_2/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_2/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 63.9449\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 16.0758\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.8458\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4864\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2418\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2049\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1382\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1344\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1109\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0700\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0691\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0770\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0580\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0561\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0661\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0569\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0551\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0662\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0555\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0557\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0556\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0553\n",
      "Original [on, off]: [0.5704856 0.9346351]\n",
      "Predicted [on, off]:  [[0.800534   0.23313037]]\n",
      "Tensor(\"lambda_3/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_3/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_2/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_3/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_3/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 75.3146\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 36.4644\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.4146\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5085\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3996\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2315\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2227\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1194\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0875\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0963\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0645\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0541\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0542\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0534\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0534\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0536\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0533\n",
      "Original [on, off]: [0.64221007 1.        ]\n",
      "Predicted [on, off]:  [[0.7508974  0.16742703]]\n"
     ]
    }
   ],
   "source": [
    "optimized_pwms = [] # store the probabilities\n",
    "optimized_seqs = [] # store the converted sequences to be tested \n",
    "predicted_targets = [] # store the original and predicted target values \n",
    "for idx, (toehold_seq, original_out) in enumerate(zip(toehold_seqs, y)): \n",
    "    optimized_pwm, optimized_onehot, predicted_out = run_gradient_ascent(toehold_seq, original_out)\n",
    "    optimized_pwms.append(np.reshape(optimized_pwm, [59, 4]))\n",
    "    predicted_targets.append(predicted_out)\n",
    "    new_seq = invert_onehot(np.reshape(optimized_onehot, [59,4]))\n",
    "    optimized_seqs.append(new_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8. Save modified toeholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['new_switch'] = optimized_seqs\n",
    "data_df['predicted_onoff'] = predicted_targets\n",
    "data_df['optimized_pwm'] = optimized_pwms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(data_dir + 'optimized_toeholds_gradascent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
