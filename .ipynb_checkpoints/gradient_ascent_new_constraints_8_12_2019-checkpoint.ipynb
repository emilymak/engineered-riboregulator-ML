{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: use gradient ascent to design better toeholds w/in biological constaints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import statements \n",
    "\n",
    "import os\n",
    "#disable CUDA\n",
    "\n",
    "import platform\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import keras \n",
    "%matplotlib inline\n",
    "\n",
    "# some visualization imports\n",
    "from vis.visualization import visualize_saliency\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "import logomaker as lm\n",
    "\n",
    "# various imports for the keras model\n",
    "from keras.layers.core import Permute\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import keras as keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import metrics as metrics\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Conv1D, Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# evaluate performance w/ on and off regression separately \n",
    "from scipy.stats import pearsonr, spearmanr \n",
    "\n",
    "# imports for the grid search and kfold CV\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# data one-hot encoding imports (help from Luis)\n",
    "from pysster.One_Hot_Encoder import One_Hot_Encoder\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# seq prop \n",
    "import isolearn.keras as iso\n",
    "import numpy as np\n",
    "\n",
    "from seqprop.visualization import *\n",
    "from seqprop.generator import *\n",
    "from seqprop.predictor import *\n",
    "from seqprop.optimizer import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load in data. Filter and sample to avoid bias from expiremental errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../data/newQC_toehold_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a036e6472810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# diff sheets, so need to read i/n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'newQC_toehold_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../data/newQC_toehold_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "data_dir = '../data/'#./data/'\n",
    "# diff sheets, so need to read i/n \n",
    "file_name = 'newQC_toehold_data.csv'\n",
    "data_df = pd.read_csv(data_dir + file_name,sep=',')\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_cutoff=1.1\n",
    "data_df = data_df[data_df['on_qc'] >= qc_cutoff].reset_index()\n",
    "data_df = data_df[data_df['off_qc'] >= qc_cutoff].reset_index()\n",
    "toehold_seqs = data_df['switch_sequence']\n",
    "seq_len = len(toehold_seqs[0])\n",
    "print('Number of remaining sequences: ', len(data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now downsample data to avoid bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "on_value_bin_labels = np.arange(1000)\n",
    "on_value_bins = pd.cut(data_df['on_value'], bins=1000, labels=on_value_bin_labels)\n",
    "bin_floor_on = math.floor(data_df['on_value'].value_counts(bins=1000).mean())\n",
    "\n",
    "\n",
    "off_value_bin_labels = np.arange(1000)\n",
    "off_value_bins = pd.cut(data_df['off_value'], bins=1000, labels=off_value_bin_labels)\n",
    "bin_floor_off = math.floor(data_df['off_value'].value_counts(bins=1000).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the 1000 bin counts and preventing no more than \n",
    "# the mean number of counts in each bin, then adding all of the indicies\n",
    "# of the bins to a list for the on and off values\n",
    "sample_ids_on = []\n",
    "for bin_label in on_value_bin_labels:\n",
    "    bin_indices = on_value_bins[on_value_bins == bin_label].index\n",
    "    bin_num = bin_indices.size\n",
    "    if bin_num > bin_floor_on:\n",
    "        sample = np.random.choice(bin_indices, size=bin_floor_on, replace=False)\n",
    "    else:\n",
    "        sample = bin_indices\n",
    "    sample_ids_on.append(sample.tolist())  \n",
    "\n",
    "sample_ids_off = []\n",
    "for bin_label in off_value_bin_labels:\n",
    "    bin_indices = off_value_bins[off_value_bins == bin_label].index\n",
    "    bin_num = bin_indices.size\n",
    "    if bin_num > bin_floor_off:\n",
    "        sample = np.random.choice(bin_indices, size=bin_floor_off, replace=False)\n",
    "    else:\n",
    "        sample = bin_indices\n",
    "    sample_ids_off.append(sample.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking down list of lists into one list\n",
    "sample_on = itertools.chain.from_iterable(sample_ids_on)\n",
    "sample_off = itertools.chain.from_iterable(sample_ids_off)\n",
    "\n",
    "# take intersection of sample_ids_on and sample_ids_off \n",
    "sample_ids_union = set(sample_on).union(sample_off)\n",
    "sub_df = data_df.loc[sample_ids_union].reset_index(drop=True)\n",
    "\n",
    "print('New number of remaining seqs:', len(sub_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameters to match original (in order to not break later code w/ new sampling)\n",
    "data_df = sub_df\n",
    "toehold_seqs = data_df['switch_sequence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Transform Data. One-hot encode sequences and extact target on and off values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alph_letters = sorted('ATCG')\n",
    "alph = list(alph_letters)\n",
    "\n",
    "# one-hot encode\n",
    "# modified code from Luis to get correct format for TPOT w/ our nt seq\n",
    "# use pysster (very fast and simple encoding)  \n",
    "one = One_Hot_Encoder(alph_letters)\n",
    "def _get_one_hot_encoding(seq):\n",
    "    one_hot_seq = one.encode(seq)                         \n",
    "    return one_hot_seq\n",
    "\n",
    "# now convert the data into one_hot_encoding \n",
    "input_col_name = 'switch_sequence'#'switch'\n",
    "X = np.stack(\n",
    "    [_get_one_hot_encoding(s) for s in toehold_seqs]).astype(np.float32)\n",
    "\n",
    "print('input shape: ', X.shape)\n",
    "# reformat for CNN \n",
    "alph_len = len(alph)\n",
    "seq_len = len(data_df[input_col_name][0])\n",
    "X = X.reshape(X.shape[0], seq_len, alph_len).astype('float32')\n",
    "print('modified shape: ', X.shape)\n",
    "\n",
    "y_on = np.array(data_df['on_value'].astype(np.float32))\n",
    "y_off = np.array(data_df['off_value'].astype(np.float32))\n",
    "\n",
    "# combine to both targets included\n",
    "# cols 0-1 = on bin, 2-3 = off bin \n",
    "y = np.transpose(np.array([y_on,y_off,]))\n",
    "print('target shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Load in final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "final_model_path = 'final_trained_model.h5'\n",
    "final_weights_path = 'final_trained_model_weights.h5'\n",
    "model = load_model(final_model_path)\n",
    "model.load_weights(final_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check architecture is the same\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Build model specific for seqprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://github.com/876lkj/seqprop \n",
    "\n",
    "# need to re-create EXACT SAME layers as final trained model\n",
    "# fix weights of layers so only input layer is modified\n",
    "def load_saved_predictor(model_path) :\n",
    "\n",
    "    saved_model = load_model(model_path)\n",
    "\n",
    "    def _initialize_predictor_weights(predictor_model, saved_model=saved_model) :\n",
    "        #Load pre-trained model\n",
    "    \n",
    "        predictor_model.get_layer('conv_0').set_weights(saved_model.get_layer('conv_0').get_weights())\n",
    "        predictor_model.get_layer('conv_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('conv_1').set_weights(saved_model.get_layer('conv_1').get_weights())\n",
    "        predictor_model.get_layer('conv_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_0').set_weights(saved_model.get_layer('dense_0').get_weights())\n",
    "        predictor_model.get_layer('dense_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_1').set_weights(saved_model.get_layer('dense_1').get_weights())\n",
    "        predictor_model.get_layer('dense_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_2').set_weights(saved_model.get_layer('dense_2').get_weights())\n",
    "        predictor_model.get_layer('dense_2').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('on_output').set_weights(saved_model.get_layer('on_output').get_weights())\n",
    "        predictor_model.get_layer('on_output').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('off_output').set_weights(saved_model.get_layer('off_output').get_weights())\n",
    "        predictor_model.get_layer('off_output').trainable = False\n",
    "\n",
    "    def _load_predictor_func(sequence_input) :\n",
    "        # input space parameters \n",
    "        seq_length = 59\n",
    "        num_letters = 4 # num nt \n",
    "        # expanded version b/c seqprop built for 2d \n",
    "        seq_input_shape = (seq_len, num_letters, 1) # modified\n",
    "\n",
    "        #define new model definition (same architecture except modified input)\n",
    "        dropout_rate=0.2\n",
    "        reg_coeff= 0.0\n",
    "        hidden_layer_choices = {5: (150, 60, 15), 10: (300, 100, 30), 15: (400,150, 30),}\n",
    "        conv_layer_parameters = [(5,10), (5,10),]\n",
    "        hidden_layers = hidden_layer_choices[10]\n",
    "        \n",
    "        #expanded_input = Input(shape=seq_input_shape,name='new_input')\n",
    "        reshaped_input = Reshape(target_shape=(seq_len, num_letters),name='reshaped_input')(sequence_input)#(expanded_input)        #(kernel_width, num_filters) = conv_layer_parameters\n",
    "        prior_layer = reshaped_input \n",
    "        for idx, (kernel_width, num_filters) in enumerate(conv_layer_parameters):\n",
    "            conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_width, padding='same', name='conv_'+str(idx))(prior_layer) # mimic a kmer\n",
    "            prior_layer = conv_layer\n",
    "        H = Flatten(name='flatten')(prior_layer)\n",
    "        for idx,h in enumerate(hidden_layers): \n",
    "            H = Dropout(dropout_rate, name='dropout_'+str(idx))(H)\n",
    "            H = Dense(h, activation='relu', kernel_regularizer=l2(reg_coeff), name='dense_'+str(idx))(H)\n",
    "        out_on = Dense(1,activation=\"linear\",name='on_output')(H)\n",
    "        out_off = Dense(1, activation='linear', name='off_output')(H)\n",
    "        on_off_out = Concatenate(name='on_of_output')([out_on,out_off])\n",
    "        \n",
    "        predictor_inputs = []#[lib_input, distal_pas_input]\n",
    "        predictor_outputs = [on_off_out]#[out]#[plasmid_out_iso, plasmid_out_cut, plasmid_score_iso, plasmid_score_cut]\n",
    "\n",
    "        return predictor_inputs, predictor_outputs, _initialize_predictor_weights\n",
    "\n",
    "    return _load_predictor_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. Set-up gradient ascent workflow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seed input which we will modify \n",
    "num_samples = 1\n",
    "seed_input_idxs = list(np.random.choice(range(len(toehold_seqs)), num_samples, replace=False))\n",
    "templates = [toehold_seqs[seed_input] for seed_input in seed_input_idxs]\n",
    "\n",
    "# template specifying what to modify and what not (biological constaints)\n",
    "switch = 'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
    "rbs = 'AACAGAGGAGA'\n",
    "start_codon = 'ATG'\n",
    "stem1 = 'NNNNNN'#'XXXXXX'\n",
    "stem2 = 'NNNNNNNNN'#'XXXXXXXXX'\n",
    "\n",
    "bio_constraints = switch + rbs + stem1 + start_codon + stem2 \n",
    "\n",
    "# build generator network\n",
    "_, seqprop_generator = build_generator(seq_length=seq_len, n_sequences=num_samples, batch_normalize_pwm=True,init_sequences = templates,\n",
    "                                      sequence_templates=bio_constraints)# batch_normalize_pwm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build predictor network and hook it on the generator PWM output tensor\n",
    "model_path = 'final_trained_model.h5'\n",
    "_, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(model_path), n_sequences=num_samples, eval_mode='pwm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL!!!! build loss function\n",
    "# ensure biological constraints are satisfied per sequence\n",
    "\n",
    "def stem_base_pairing(pwm): \n",
    "    # ensure that location of 1s in switch region matches reverse complement of stem\n",
    "    \n",
    "    def reverse_complement(base_index): \n",
    "        # ACGT = alphabett\n",
    "        if base_index == 0: return 3\n",
    "        elif base_index == 1: return 2 \n",
    "        elif base_index == 2: return 1 \n",
    "        elif base_index == 3: return 0\n",
    "    \n",
    "    # reverse complement is reverse over axis of one-hot encoded nt \n",
    "    nt_reversed = K.reverse(pwm, axes=2)\n",
    "    \n",
    "    stem1_score = 6 - K.sum(pwm[:, 24, :, 0]*nt_reversed[:, 41,:, 0] + pwm[:, 25, :, 0]*nt_reversed[:, 42, :, 0]+ pwm[:,26, :, 0]*nt_reversed[:, 43, :, 0] + pwm[:, 27, :, 0]*nt_reversed[:, 44, :, 0] + pwm[:, 28, :, 0]*nt_reversed[:, 45, :, 0]+ pwm[:, 29, :, 0]*nt_reversed[:, 46, :, 0])\n",
    "    \n",
    "    stem2_score = 9 - K.sum(pwm[:, 12, :, 0]*nt_reversed[:, 50, :, 0] + pwm[:, 13, :, 0]*nt_reversed[:, 51, :, 0]+ pwm[:, 14, :, 0]*nt_reversed[:, 52, :, 0]+ pwm[:, 15, :, 0]*nt_reversed[:, 53, :, 0] + pwm[:, 16, :, 0]*nt_reversed[:, 54, :, 0] + pwm[:, 17, :, 0]*nt_reversed[:,55, :, 0]+ pwm[:, 18,:, 0]*nt_reversed[:, 56, :, 0] + pwm[:, 19, :, 0]*nt_reversed[:,57, :, 0] + pwm[:, 20, :, 0]*nt_reversed[:, 58, :, 0])\n",
    "    \n",
    "    return 10*stem1_score + 10*stem2_score\n",
    "\n",
    "\n",
    "def loss_func(predictor_outputs) :\n",
    "    pwm_logits, pwm, sampled_pwm, predicted_out = predictor_outputs\n",
    "    #print(predictor_outputs)\n",
    "  \n",
    "    #Create target constant -- want predicted value for modified input to be close to target input \n",
    "    target_out = K.tile(K.constant(target), (K.shape(sampled_pwm)[0], 1))\n",
    "    target_cost = (target_out - predicted_out)**2\n",
    "    print(target_out, target_cost, predicted_out)\n",
    "    # posssible: INSTEAD OF PWM COST, have cost relative to consensus sequence!! \n",
    "    # i.e. some distance from the consensus seq \n",
    "    base_pairing_cost = stem_base_pairing(sampled_pwm)\n",
    "    print(base_pairing_cost)\n",
    "    print(K.mean(target_cost + base_pairing_cost, axis=-1))\n",
    "    return K.mean(target_cost + base_pairing_cost, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. Run gradient ascent on the specified seed input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target on, off values \n",
    "target_on = 0.99\n",
    "target_off = 0.0001\n",
    "target = [[target_on,target_off], ] \n",
    "\n",
    "#Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "_, loss_model = build_loss_model(seqprop_predictor, loss_func, )\n",
    "\n",
    "#Specify Optimizer to use\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "#Compile Loss Model (Minimize self)\n",
    "loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "#Fit Loss Model\n",
    "#seed_input = np.reshape([X[0]], [1,59,4,1]) # any input toehold to be modified\n",
    "\n",
    "callbacks =[\n",
    "            EarlyStopping(monitor='loss', min_delta=0.001, patience=5, verbose=0, mode='auto'),\n",
    "            #SeqPropMonitor(predictor=seqprop_predictor)#, plot_every_epoch=True, track_every_step=True, )#cse_start_pos=70, isoform_start=target_cut, isoform_end=target_cut+1, pwm_start=70-40, pwm_end=76+50, sequence_template=sequence_template, plot_pwm_indices=[0])\n",
    "        ]\n",
    "\n",
    "\n",
    "num_epochs=50\n",
    "train_history = loss_model.fit([], np.ones((1, 1)), epochs=num_epochs, steps_per_epoch=1000, callbacks=callbacks)\n",
    "\n",
    "#Retrieve optimized PWMs and predicted (optimized) target\n",
    "_, optimized_pwm, optimized_onehot, predicted_out = seqprop_predictor.predict(x=None, steps=1)\n",
    "print('Predicted output: ', predicted_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7. Visualize modifications and re-designed toehold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original toehold \n",
    "seed_input_idx=0 # map back to which toehold was used as the seed for this run\n",
    "true_logo = lm.Logo(pd.DataFrame(X[seed_input_idxs[seed_input_idx]],columns=alph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the maximum nt at each position to get the new toehold \n",
    "logo_oh = lm.Logo(pd.DataFrame(np.reshape(optimized_onehot, [59, 4]),columns=alph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de novo pwm to achieve output \n",
    "# depicts uncertainty in positions\n",
    "opt_seq = np.reshape(optimized_pwm, [59, 4])\n",
    "logo_pwm = lm.Logo(pd.DataFrame(opt_seq,columns=alph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_pwm[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(K.reverse(optimized_pwm[0,:,:,0],axes=0).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
