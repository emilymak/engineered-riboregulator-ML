{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Optimize the toehold sequences with gradient ascent to improve the ON/OFF ratio.\n",
    "\n",
    "### Instructions: Please change the file_name in the second code block to sequences you are interested in redesigning. The format should be a .csv file with at least three columns: a switch_sequence column with the original DNA sequence of the toehold; an on_value column with the ON value of the switch (can be predicted if in silico); and an off_value column with the OFF value of the switch (again, can be predicted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import statements \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "%matplotlib inline\n",
    "\n",
    "import keras as keras\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from pysster.One_Hot_Encoder import One_Hot_Encoder\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import isolearn.keras as iso\n",
    "from seqprop import *\n",
    "from seqprop.generator import *\n",
    "from seqprop.predictor import *\n",
    "from seqprop.optimizer import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load in sequence data. \n",
    "## Change file_name here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>switch_sequence</th>\n",
       "      <th>on_value</th>\n",
       "      <th>off_value</th>\n",
       "      <th>onoff_value</th>\n",
       "      <th>on_preds</th>\n",
       "      <th>off_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACAAAAAAACAATAAAAAATAGAGAAAAAGAACAGAGGAGACTTTT...</td>\n",
       "      <td>0.428270</td>\n",
       "      <td>0.818291</td>\n",
       "      <td>-0.390021</td>\n",
       "      <td>0.521815</td>\n",
       "      <td>0.815901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATAAACAAAATGGATATTATAGACAAAAAAAACAGAGGAGATTTTT...</td>\n",
       "      <td>0.570486</td>\n",
       "      <td>0.934635</td>\n",
       "      <td>-0.364150</td>\n",
       "      <td>0.700090</td>\n",
       "      <td>0.864703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GATGTTACAAACGATAATATAGACAAAAATAACAGAGGAGAATTTT...</td>\n",
       "      <td>0.642210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.357790</td>\n",
       "      <td>0.718297</td>\n",
       "      <td>0.850942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     switch_sequence  on_value  off_value  \\\n",
       "0  ACAAAAAAACAATAAAAAATAGAGAAAAAGAACAGAGGAGACTTTT...  0.428270   0.818291   \n",
       "1  ATAAACAAAATGGATATTATAGACAAAAAAAACAGAGGAGATTTTT...  0.570486   0.934635   \n",
       "2  GATGTTACAAACGATAATATAGACAAAAATAACAGAGGAGAATTTT...  0.642210   1.000000   \n",
       "\n",
       "   onoff_value  on_preds  off_preds  \n",
       "0    -0.390021  0.521815   0.815901  \n",
       "1    -0.364150  0.700090   0.864703  \n",
       "2    -0.357790  0.718297   0.850942  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter a .csv with sequences\n",
    "data_dir = 'gradient_ascent_sequences/'\n",
    "file_name = 'worst_toehold_sequences.csv' # CHANGE FILENAME!\n",
    "data_df = pd.read_csv(data_dir + file_name,sep=',')\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toehold length:  59\n",
      "Number of sequences:  100\n"
     ]
    }
   ],
   "source": [
    "toehold_seqs = data_df['switch_sequence']\n",
    "seq_len = len(toehold_seqs[0])\n",
    "print('Toehold length: ', seq_len)\n",
    "num_seqs = len(data_df)\n",
    "print('Number of sequences: ', num_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Extract toeholds to optimize.\n",
    "### Note: 100 sequences takes ~2 hours to optimize, given compute power, so simplify to just 10 sequences here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences:  10\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df[0:10]\n",
    "toehold_seqs = data_df['switch_sequence']\n",
    "print('Number of sequences: ', len(toehold_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Transform Data. One-hot encode sequences and extact target on and off values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (10, 59, 4)\n",
      "target shape:  (10, 2)\n"
     ]
    }
   ],
   "source": [
    "# create DNA alphabet- may need to change if you have RNA toeholds. Just change to 'AUCG' in the first line\n",
    "alph_letters = sorted('ATCG')\n",
    "alph = list(alph_letters)\n",
    "\n",
    "# one-hot encode with pysster (very fast and simple encoding)  \n",
    "one = One_Hot_Encoder(alph_letters)\n",
    "def _get_one_hot_encoding(seq):\n",
    "    one_hot_seq = one.encode(seq)                         \n",
    "    return one_hot_seq\n",
    "\n",
    "# now convert the data into one_hot_encoding \n",
    "input_col_name = 'switch_sequence'\n",
    "X = np.stack([_get_one_hot_encoding(s) for s in toehold_seqs]).astype(np.float32)\n",
    "print('input shape: ', X.shape)\n",
    "\n",
    "# now set y as the on and off values\n",
    "y_on = np.array(data_df['on_value'].astype(np.float32))\n",
    "y_off = np.array(data_df['off_value'].astype(np.float32))\n",
    "y = np.transpose(np.array([y_on,y_off,]))\n",
    "print('target shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Load in final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'trained_model/'\n",
    "final_model_path = model_dir + 'final_trained_model.h5'\n",
    "final_weights_path = model_dir + 'final_trained_model_weights.h5'\n",
    "model = load_model(final_model_path)\n",
    "model.load_weights(final_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 59, 4)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv1D)                 (None, 59, 10)       210         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv1D)                 (None, 59, 5)        155         conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 295)          0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 295)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 150)          44400       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 150)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 60)           9060        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 60)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15)           915         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "on_output (Dense)               (None, 1)            16          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "off_output (Dense)              (None, 1)            16          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 54,772\n",
      "Trainable params: 54,772\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# visually inspect architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. Build model specific for seqprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://github.com/876lkj/seqprop \n",
    "\n",
    "# need to re-create EXACT SAME layers as final trained model\n",
    "# fix weights of layers so only input layer is modified\n",
    "def load_saved_predictor(model_path) :\n",
    "\n",
    "    saved_model = load_model(model_path)\n",
    "\n",
    "    def _initialize_predictor_weights(predictor_model, saved_model=saved_model) :\n",
    "        #Load pre-trained model\n",
    "    \n",
    "        predictor_model.get_layer('conv_0').set_weights(saved_model.get_layer('conv_0').get_weights())\n",
    "        predictor_model.get_layer('conv_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('conv_1').set_weights(saved_model.get_layer('conv_1').get_weights())\n",
    "        predictor_model.get_layer('conv_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_0').set_weights(saved_model.get_layer('dense_0').get_weights())\n",
    "        predictor_model.get_layer('dense_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_1').set_weights(saved_model.get_layer('dense_1').get_weights())\n",
    "        predictor_model.get_layer('dense_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_2').set_weights(saved_model.get_layer('dense_2').get_weights())\n",
    "        predictor_model.get_layer('dense_2').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('on_output').set_weights(saved_model.get_layer('on_output').get_weights())\n",
    "        predictor_model.get_layer('on_output').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('off_output').set_weights(saved_model.get_layer('off_output').get_weights())\n",
    "        predictor_model.get_layer('off_output').trainable = False\n",
    "\n",
    "    def _load_predictor_func(sequence_input) :\n",
    "        # input space parameters \n",
    "        seq_length = 59\n",
    "        num_letters = 4 # num nt \n",
    "        # expanded version b/c seqprop built for 2d \n",
    "        seq_input_shape = (seq_len, num_letters, 1) # modified\n",
    "\n",
    "        #define new model definition (same architecture except modified input)\n",
    "        dropout_rate=0.1\n",
    "        reg_coeff= 0.0001\n",
    "        hidden_layer_choices = {5: (150, 60, 15), 10: (300, 100, 30), 15: (400,150, 30),}\n",
    "        conv_layer_parameters = [(5,10), (3,5),]\n",
    "        hidden_layers = hidden_layer_choices[5]\n",
    "        \n",
    "        #expanded_input = Input(shape=seq_input_shape,name='new_input')\n",
    "        reshaped_input = Reshape(target_shape=(seq_len, num_letters),name='reshaped_input')(sequence_input)#(expanded_input)        #(kernel_width, num_filters) = conv_layer_parameters\n",
    "        prior_layer = reshaped_input \n",
    "        for idx, (kernel_width, num_filters) in enumerate(conv_layer_parameters):\n",
    "            conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_width, padding='same', name='conv_'+str(idx))(prior_layer) # mimic a kmer\n",
    "            prior_layer = conv_layer\n",
    "        H = Flatten(name='flatten')(prior_layer)\n",
    "        for idx,h in enumerate(hidden_layers): \n",
    "            H = Dropout(dropout_rate, name='dropout_'+str(idx))(H)\n",
    "            H = Dense(h, activation='relu', kernel_regularizer=l2(reg_coeff), name='dense_'+str(idx))(H)\n",
    "        out_on = Dense(1,activation=\"linear\",name='on_output')(H)\n",
    "        out_off = Dense(1, activation='linear', name='off_output')(H)\n",
    "        on_off_out = Concatenate(name='on_of_output')([out_on,out_off])\n",
    "        \n",
    "        predictor_inputs = []\n",
    "        predictor_outputs = [on_off_out]\n",
    "\n",
    "        return predictor_inputs, predictor_outputs, _initialize_predictor_weights\n",
    "\n",
    "    return _load_predictor_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. Set-up gradient ascent workflow. Convert to callable function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants \n",
    "\n",
    "# get seed input which we will modify \n",
    "num_samples = 1\n",
    "\n",
    "# template specifying what to modify and what not (biological constaints)\n",
    "switch = 'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
    "rbs = 'AACAGAGGAGA'\n",
    "start_codon = 'ATG'\n",
    "stem1 = 'NNNNNN'#'XXXXXX'\n",
    "stem2 = 'NNNNNNNNN'#'XXXXXXXXX'\n",
    "\n",
    "bio_constraints = switch + rbs + stem1 + start_codon + stem2 \n",
    "\n",
    "# define target on, off values \n",
    "target_on = 0.99\n",
    "target_off = 0.001\n",
    "target = [[target_on,target_off], ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loss function\n",
    "# ensure biological constraints are satisfied per sequence\n",
    "\n",
    "def stem_base_pairing(pwm): \n",
    "    # ensure that location of 1s in switch region matches reverse complement of stem\n",
    "    \n",
    "    def reverse_complement(base_index): \n",
    "        # ACGT = alphabett\n",
    "        if base_index == 0: return 3\n",
    "        elif base_index == 1: return 2 \n",
    "        elif base_index == 2: return 1 \n",
    "        elif base_index == 3: return 0\n",
    "    \n",
    "    # reverse complement is reverse over axis of one-hot encoded nt \n",
    "    nt_reversed = K.reverse(pwm, axes=2)\n",
    "    stem1_score = 6 - K.sum(pwm[:, 24, :, 0]*nt_reversed[:, 41,:, 0] + pwm[:, 25, :, 0]*nt_reversed[:, 42, :, 0]+ pwm[:,26, :, 0]*nt_reversed[:, 43, :, 0] + pwm[:, 27, :, 0]*nt_reversed[:, 44, :, 0] + pwm[:, 28, :, 0]*nt_reversed[:, 45, :, 0]+ pwm[:, 29, :, 0]*nt_reversed[:, 46, :, 0])\n",
    "    stem2_score = 9 - K.sum(pwm[:, 12, :, 0]*nt_reversed[:, 50, :, 0] + pwm[:, 13, :, 0]*nt_reversed[:, 51, :, 0]+ pwm[:, 14, :, 0]*nt_reversed[:, 52, :, 0]+ pwm[:, 15, :, 0]*nt_reversed[:, 53, :, 0] + pwm[:, 16, :, 0]*nt_reversed[:, 54, :, 0] + pwm[:, 17, :, 0]*nt_reversed[:,55, :, 0]+ pwm[:, 18,:, 0]*nt_reversed[:, 56, :, 0] + pwm[:, 19, :, 0]*nt_reversed[:,57, :, 0] + pwm[:, 20, :, 0]*nt_reversed[:, 58, :, 0])\n",
    "    return 10*stem1_score + 10*stem2_score\n",
    "\n",
    "def loss_func(predictor_outputs) :\n",
    "    pwm_logits, pwm, sampled_pwm, predicted_out = predictor_outputs\n",
    "  \n",
    "    #Create target constant -- want predicted value for modified input to be close to target input \n",
    "    target_out = K.tile(K.constant(target), (K.shape(sampled_pwm)[0], 1))\n",
    "    target_cost = (target_out - predicted_out)**2\n",
    "    print(target_out, target_cost, predicted_out)\n",
    "    base_pairing_cost = stem_base_pairing(sampled_pwm)\n",
    "    print(base_pairing_cost)\n",
    "    print(K.mean(target_cost + base_pairing_cost, axis=-1))\n",
    "    return K.mean(target_cost + base_pairing_cost, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_ascent(input_toehold_seq, original_out):\n",
    "\n",
    "    # build generator network\n",
    "    _, seqprop_generator = build_generator(seq_length=seq_len, n_sequences=num_samples, batch_normalize_pwm=True,init_sequences = [input_toehold_seq],\n",
    "                                          sequence_templates=bio_constraints)# batch_normalize_pwm=True)\n",
    "    \n",
    "    # build predictor network and hook it on the generator PWM output tensor\n",
    "    _, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(final_model_path), n_sequences=num_samples, eval_mode='pwm')\n",
    "\n",
    "    #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "    _, loss_model = build_loss_model(seqprop_predictor, loss_func, )\n",
    "\n",
    "    #Specify Optimizer to use\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    #Compile Loss Model (Minimize self)\n",
    "    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "    #Fit Loss Model\n",
    "    #seed_input = np.reshape([X[0]], [1,59,4,1]) # any input toehold to be modified\n",
    "\n",
    "    callbacks =[\n",
    "                EarlyStopping(monitor='loss', min_delta=0.001, patience=5, verbose=0, mode='auto'),\n",
    "                #SeqPropMonitor(predictor=seqprop_predictor)#, plot_every_epoch=True, track_every_step=True, )#cse_start_pos=70, isoform_start=target_cut, isoform_end=target_cut+1, pwm_start=70-40, pwm_end=76+50, sequence_template=sequence_template, plot_pwm_indices=[0])\n",
    "            ]\n",
    "\n",
    "\n",
    "    num_epochs=50\n",
    "    train_history = loss_model.fit([], np.ones((1, 1)), epochs=num_epochs, steps_per_epoch=1000, callbacks=callbacks)\n",
    "\n",
    "    #Retrieve optimized PWMs and predicted (optimized) target\n",
    "    _, optimized_pwm, optimized_onehot, predicted_out = seqprop_predictor.predict(x=None, steps=1)\n",
    "    print('Original [on, off]:', original_out)\n",
    "    print('Predicted [on, off]: ', predicted_out)\n",
    "    \n",
    "    return optimized_pwm, optimized_onehot, predicted_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7. Run gradient ascent on the specified seed inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_onehot(oh_seq): \n",
    "    return ''.join(alph[idx] for idx in np.argmax(oh_seq,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/seqprop-0.1-py3.7.egg/seqprop/generator/seqprop_generator.py:26: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "Tensor(\"lambda_1/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_1/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_1/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_1/Mean:0\", shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 51.1585\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 11.4761\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0053\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4768\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3915\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3102\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2231\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1989A: 0s \n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1866\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1556\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1634\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1423\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1520\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1416\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1424\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1416\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1423\n",
      "Original [on, off]: [0.42827037 0.81829125]\n",
      "Predicted [on, off]:  [[0.5468402  0.23553035]]\n",
      "Tensor(\"lambda_2/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_2/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_1/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_2/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_2/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 63.6127\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 18.9939\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.2052\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4614\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2800\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2236\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0946\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1227A: 0s - loss: 0.12\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0801\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0588\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0683\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0556\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0461\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0456\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0462\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0455\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0456\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0459\n",
      "Original [on, off]: [0.5704856 0.9346351]\n",
      "Predicted [on, off]:  [[0.79738045 0.18310557]]\n",
      "Tensor(\"lambda_3/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_3/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_2/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_3/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_3/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 75.9313\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 35.6750\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.8173\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4643\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2774\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1318\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1479\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1036\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0592\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0662\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0664\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0552\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0649\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0657\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0652\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0548\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0540\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0538\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0548\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0541\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0538\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0539\n",
      "Original [on, off]: [0.64221007 1.        ]\n",
      "Predicted [on, off]:  [[0.7507162  0.18408066]]\n",
      "Tensor(\"lambda_4/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_4/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_3/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_4/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_4/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 76.6811\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 41.3633\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 9.3852\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.9069\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5546\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4879\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3870\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3257\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3361\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3073\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2877\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2872\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2895\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2781\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2789\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2776\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2673\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2753\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2879\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2755\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2689\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2787\n",
      "Original [on, off]: [0.6502825 1.       ]\n",
      "Predicted [on, off]:  [[0.18971056 0.06090861]]\n",
      "Tensor(\"lambda_5/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_5/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_4/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_5/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_5/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 65.3743\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 20.1071\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 7.7317\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.9089\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3898\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1703\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1980\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1678\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0915\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0693\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0473\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0457\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0554\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0445\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0445\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0442\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0443\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0439\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0442\n",
      "Original [on, off]: [0.66599333 1.        ]\n",
      "Predicted [on, off]:  [[0.833041  0.2129975]]\n",
      "Tensor(\"lambda_6/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_6/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_5/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_6/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_6/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 69.6489\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 35.7555\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 5.8625\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6293\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2894\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3030\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2072\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1240\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0716\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0796\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0689\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0587\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0678\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0481\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0477\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0586\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0475\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0477\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0471\n",
      "Original [on, off]: [0.65388757 0.98592895]\n",
      "Predicted [on, off]:  [[0.8484731  0.21843985]]\n",
      "Tensor(\"lambda_7/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_7/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_6/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_7/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_7/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 39.7584\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 5.4104\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.3820\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6101\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3403\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2787\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1312\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.1363\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0625\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0617\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0603\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0595\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0788\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0493\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0488\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0489\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0586\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0484\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0476\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0488\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0480A: 0s - loss: 0.0\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0475\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0472\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0481\n",
      "Original [on, off]: [0.4977509  0.80630714]\n",
      "Predicted [on, off]:  [[0.7668601  0.15529537]]\n",
      "Tensor(\"lambda_8/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_8/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_7/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_8/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_8/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 70.6279\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 24.0194\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.4528\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4600\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3821\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2323\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1768\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1198\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1073\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1043\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1032\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0826\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0929\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0912\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0806\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0803\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0801\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0800\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0808\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0806\n",
      "Original [on, off]: [0.6174001  0.91486585]\n",
      "Predicted [on, off]:  [[0.64913106 0.16595224]]\n",
      "Tensor(\"lambda_9/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_9/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_8/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_9/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_9/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 57.2815\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 10.9339\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.8511\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5704\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2505A: 0s - loss:\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1325\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1197\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0754\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0731\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0515\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0305\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0299\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0489\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0291\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0291\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0289\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0288\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0288\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0289\n",
      "Original [on, off]: [0.70662045 1.        ]\n",
      "Predicted [on, off]:  [[0.8467668  0.13995245]]\n",
      "Tensor(\"lambda_10/Tile:0\", shape=(1, 2), dtype=float32) Tensor(\"lambda_10/pow:0\", shape=(1, 2), dtype=float32) Tensor(\"on_of_output_9/concat:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"lambda_10/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_10/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 69.2767\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 23.2254\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.7529\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6838\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5679\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4329\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4584\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4326\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3884\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3672\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3753\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3642\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3539\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3658\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3546\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3553\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3546\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3526\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3553\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3540\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3543\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3549\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3549\n",
      "Original [on, off]: [0.683358   0.97379094]\n",
      "Predicted [on, off]:  [[0.15902367 0.0800747 ]]\n"
     ]
    }
   ],
   "source": [
    "optimized_pwms = [] # store the probabilities\n",
    "optimized_seqs = [] # store the converted sequences to be tested \n",
    "predicted_targets = [] # store the original and predicted target values \n",
    "for idx, (toehold_seq, original_out) in enumerate(zip(toehold_seqs, y)): \n",
    "    optimized_pwm, optimized_onehot, predicted_out = run_gradient_ascent(toehold_seq, original_out)\n",
    "    optimized_pwms.append(np.reshape(optimized_pwm, [59, 4]))\n",
    "    predicted_targets.append(predicted_out)\n",
    "    new_seq = invert_onehot(np.reshape(optimized_onehot, [59,4]))\n",
    "    optimized_seqs.append(new_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8. Save modified toeholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['new_switch'] = optimized_seqs\n",
    "data_df['predicted_onoff'] = predicted_targets\n",
    "data_df['optimized_pwm'] = optimized_pwms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(data_dir + 'optimized_toeholds_gradascent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.7.1\n",
      "anndata==0.6.19\n",
      "appdirs==1.4.3\n",
      "appnope==0.1.0\n",
      "asgiref==3.2.3\n",
      "astor==0.7.1\n",
      "atomicwrites==1.3.0\n",
      "attrs==19.1.0\n",
      "backcall==0.1.0\n",
      "backports.csv==1.0.7\n",
      "biopython==1.73\n",
      "bleach==3.1.0\n",
      "certifi==2018.11.29\n",
      "cffi==1.12.3\n",
      "chardet==3.0.4\n",
      "chart-studio==1.0.0\n",
      "Click==7.0\n",
      "cycler==0.10.0\n",
      "DCA==0.2.2\n",
      "decorator==4.4.0\n",
      "defusedxml==0.5.0\n",
      "Deprecated==1.2.4\n",
      "dill==0.3.0\n",
      "dj-database-url==0.5.0\n",
      "Django==3.0.1\n",
      "entrypoints==0.3\n",
      "Flask==1.1.1\n",
      "forgi==2.0.2\n",
      "future==0.17.1\n",
      "gast==0.2.2\n",
      "GEOparse==1.1.0\n",
      "graphtools==1.1.0\n",
      "grpcio==1.19.0\n",
      "h5py==2.9.0\n",
      "hyperopt==0.1.2\n",
      "idna==2.8\n",
      "imageio==2.5.0\n",
      "importlib-metadata==0.22\n",
      "ipykernel==5.1.0\n",
      "ipython==7.3.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.4.2\n",
      "isolearn==0.2.1\n",
      "itsdangerous==1.1.0\n",
      "jedi==0.13.3\n",
      "Jinja2==2.10.3\n",
      "joblib==0.13.2\n",
      "jsonschema==3.0.1\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.2.4\n",
      "jupyter-console==6.0.0\n",
      "jupyter-core==4.4.0\n",
      "Keras==2.2.4\n",
      "Keras-Applications==1.0.7\n",
      "Keras-Preprocessing==1.0.9\n",
      "keras-vis==0.5.0\n",
      "keyring==12.0.2\n",
      "kiwisolver==1.0.1\n",
      "kopt==0.1.0\n",
      "llvmlite==0.28.0\n",
      "logging-exceptions==0.1.8\n",
      "logomaker==0.8\n",
      "magic-impute==1.5.3\n",
      "Markdown==3.0.1\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.0.3\n",
      "mistune==0.8.4\n",
      "mlens==0.2.3\n",
      "mock==2.0.0\n",
      "more-itertools==7.2.0\n",
      "msgpack==0.6.1\n",
      "natsort==6.0.0\n",
      "nbconvert==5.4.1\n",
      "nbformat==4.4.0\n",
      "networkx==2.2\n",
      "notebook==5.7.6\n",
      "numba==0.43.1\n",
      "numexpr==2.6.9\n",
      "numpy==1.15.4\n",
      "opencv-python==4.1.1.26\n",
      "packaging==19.1\n",
      "pandas==0.23.4\n",
      "pandocfilters==1.4.2\n",
      "parso==0.3.4\n",
      "patsy==0.5.1\n",
      "pbr==5.1.3\n",
      "pexpect==4.6.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==6.1.0\n",
      "plotly==4.1.0\n",
      "pluggy==0.13.0\n",
      "prometheus-client==0.6.0\n",
      "prompt-toolkit==2.0.9\n",
      "protobuf==3.7.0\n",
      "ptyprocess==0.6.0\n",
      "PubChemPy==1.0.4\n",
      "py==1.8.0\n",
      "pycparser==2.19\n",
      "pydot==1.4.1\n",
      "Pygments==2.3.1\n",
      "PyGSP==0.5.1\n",
      "pymongo==3.7.2\n",
      "pyparsing==2.3.1\n",
      "pyrsistent==0.14.11\n",
      "pysster==1.2.1\n",
      "pytest==5.1.2\n",
      "python-dateutil==2.8.0\n",
      "pytz==2018.9\n",
      "PyWavelets==1.0.3\n",
      "PyYAML==5.1\n",
      "pyzmq==18.0.1\n",
      "qtconsole==4.4.3\n",
      "requests==2.22.0\n",
      "retrying==1.3.3\n",
      "rpy2==3.1.0\n",
      "scanpy==1.4\n",
      "scikit-image==0.15.0\n",
      "scikit-learn==0.20.1\n",
      "scipy==1.1.0\n",
      "scprep==0.11.1\n",
      "seaborn==0.9.0\n",
      "Send2Trash==1.5.0\n",
      "seqprop==0.1\n",
      "shap==0.30.1\n",
      "simplegeneric==0.8.1\n",
      "six==1.12.0\n",
      "sklearn==0.0\n",
      "sqlparse==0.3.0\n",
      "statsmodels==0.9.0\n",
      "svgwrite==1.3.1\n",
      "synapseclient==1.9.3\n",
      "tables==3.5.1\n",
      "tasklogger==0.4.2\n",
      "tensorboard==1.13.1\n",
      "tensorflow==1.13.1\n",
      "tensorflow-estimator==1.13.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.1\n",
      "testpath==0.4.2\n",
      "tornado==6.0.1\n",
      "tqdm==4.31.1\n",
      "traitlets==4.3.2\n",
      "tzlocal==2.0.0\n",
      "umap==0.1.1\n",
      "umap-learn==0.3.9\n",
      "urllib3==1.25.3\n",
      "vis==0.0.5\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.15.0\n",
      "wgetter==0.7\n",
      "whitenoise==5.0.1\n",
      "widgetsnbextension==3.4.2\n",
      "wrapt==1.11.2\n",
      "xlrd==1.2.0\n",
      "zipp==0.6.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
