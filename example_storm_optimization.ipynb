{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Optimize the toehold sequences with gradient ascent to improve the ON/OFF ratio.\n",
    "\n",
    "### Instructions: Please change the file_name in the second code block to sequences you are interested in redesigning. The format should be a .csv file with at least three columns: a switch_sequence column with the original DNA sequence of the toehold; an on_value column with the ON value of the switch (can be predicted if in silico); and an off_value column with the OFF value of the switch (again, can be predicted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import statements \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "\n",
    "import keras as keras\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from pysster.One_Hot_Encoder import One_Hot_Encoder\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import isolearn.keras as iso\n",
    "import seqprop\n",
    "from seqprop import *\n",
    "from seqprop.generator import *\n",
    "from seqprop.predictor import *\n",
    "from seqprop.optimizer import *\n",
    "# need to install seqprop as below if not already installed:\n",
    "# git clone https://github.com/johli/seqprop.git\n",
    "# cd seqprop\n",
    "# python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1a: Load in sequence data. \n",
    "### Change file_name here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>switch_sequence</th>\n",
       "      <th>on_value</th>\n",
       "      <th>off_value</th>\n",
       "      <th>onoff_value</th>\n",
       "      <th>on_preds</th>\n",
       "      <th>off_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACAAAAAAACAATAAAAAATAGAGAAAAAGAACAGAGGAGACTTTT...</td>\n",
       "      <td>0.428270</td>\n",
       "      <td>0.818291</td>\n",
       "      <td>-0.390021</td>\n",
       "      <td>0.521815</td>\n",
       "      <td>0.815901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATAAACAAAATGGATATTATAGACAAAAAAAACAGAGGAGATTTTT...</td>\n",
       "      <td>0.570486</td>\n",
       "      <td>0.934635</td>\n",
       "      <td>-0.364150</td>\n",
       "      <td>0.700090</td>\n",
       "      <td>0.864703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GATGTTACAAACGATAATATAGACAAAAATAACAGAGGAGAATTTT...</td>\n",
       "      <td>0.642210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.357790</td>\n",
       "      <td>0.718297</td>\n",
       "      <td>0.850942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     switch_sequence  on_value  off_value  \\\n",
       "0  ACAAAAAAACAATAAAAAATAGAGAAAAAGAACAGAGGAGACTTTT...  0.428270   0.818291   \n",
       "1  ATAAACAAAATGGATATTATAGACAAAAAAAACAGAGGAGATTTTT...  0.570486   0.934635   \n",
       "2  GATGTTACAAACGATAATATAGACAAAAATAACAGAGGAGAATTTT...  0.642210   1.000000   \n",
       "\n",
       "   onoff_value  on_preds  off_preds  \n",
       "0    -0.390021  0.521815   0.815901  \n",
       "1    -0.364150  0.700090   0.864703  \n",
       "2    -0.357790  0.718297   0.850942  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter a .csv with sequences\n",
    "data_dir = 'data/gradient_ascent_sequences/'\n",
    "file_name = 'worst_toehold_sequences.csv' # CHANGE FILENAME!\n",
    "data_df = pd.read_csv(data_dir + file_name,sep=',')\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toehold length:  59\n",
      "Number of sequences:  100\n"
     ]
    }
   ],
   "source": [
    "toehold_seqs = data_df['switch_sequence']\n",
    "seq_len = len(toehold_seqs[0])\n",
    "print('Toehold length: ', seq_len)\n",
    "num_seqs = len(data_df)\n",
    "print('Number of sequences: ', num_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1b: Extract toeholds to optimize.\n",
    "### Note: 20 sequences takes ~2 hours to optimize, given compute power, so simplify to just 5 sequences here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences:  2\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df[0:2]\n",
    "seqs = data_df['switch_sequence']\n",
    "print('Number of sequences: ', len(seqs))\n",
    "num_seqs = len(seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Transform Data. One-hot encode sequences and extact target on and off values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (2, 59, 4)\n"
     ]
    }
   ],
   "source": [
    "# create DNA alphabet- may need to change if you have RNA toeholds. Just change to 'AUCG' in the first line\n",
    "alph_letters = sorted('ACGT')\n",
    "alph = list(alph_letters)\n",
    "\n",
    "# one-hot encode with pysster (very fast and simple encoding)  \n",
    "one = One_Hot_Encoder(alph_letters)\n",
    "def _get_one_hot_encoding(seq):\n",
    "    one_hot_seq = one.encode(seq)                         \n",
    "    return one_hot_seq\n",
    "\n",
    "# now convert the data into one_hot_encoding \n",
    "input_col_name = 'switch_sequence'\n",
    "X = np.stack([_get_one_hot_encoding(s) for s in seqs]).astype(np.float32)\n",
    "print('input shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Load in final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'clean_figures/fig4/models/'\n",
    "final_model_path = model_dir + 'freeze_weights_tf_onoff_model.h5'\n",
    "final_weights_path = model_dir + 'freeze_weights_tf_onoff_model_weights.h5'\n",
    "model = load_model(final_model_path)\n",
    "model.load_weights(final_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target shape:  (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# use it to predict y values\n",
    "storm_pred_onoff_vals = model.predict(X)\n",
    "y = np.array(storm_pred_onoff_vals).astype(np.float32)\n",
    "print('target shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Build model specific for seqprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://github.com/876lkj/seqprop \n",
    "\n",
    "# need to re-create EXACT SAME layers as final trained model\n",
    "# fix weights of layers so only input layer is modified\n",
    "def load_saved_predictor(model_path) :\n",
    "\n",
    "    saved_model = load_model(model_path)\n",
    "\n",
    "    def _initialize_predictor_weights(predictor_model, saved_model=saved_model) :\n",
    "        #Load pre-trained model\n",
    "        predictor_model.get_layer('conv_0').set_weights(saved_model.get_layer('conv_0').get_weights())\n",
    "        predictor_model.get_layer('conv_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('conv_1').set_weights(saved_model.get_layer('conv_1').get_weights())\n",
    "        predictor_model.get_layer('conv_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_0').set_weights(saved_model.get_layer('dense_0').get_weights())\n",
    "        predictor_model.get_layer('dense_0').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_1').set_weights(saved_model.get_layer('dense_1').get_weights())\n",
    "        predictor_model.get_layer('dense_1').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('dense_2').set_weights(saved_model.get_layer('dense_2').get_weights())\n",
    "        predictor_model.get_layer('dense_2').trainable = False\n",
    "\n",
    "        predictor_model.get_layer('on_output').set_weights(saved_model.get_layer('on_output').get_weights())\n",
    "        predictor_model.get_layer('on_output').trainable = False\n",
    "\n",
    "    def _load_predictor_func(sequence_input) :\n",
    "        # input space parameters \n",
    "        seq_length = 59\n",
    "        num_letters = 4 # num nt \n",
    "        # expanded version b/c seqprop built for 2d \n",
    "        seq_input_shape = (seq_len, num_letters, 1) # modified\n",
    "\n",
    "        #define new model definition (same architecture except modified input)\n",
    "        dropout_rate=0.1\n",
    "        reg_coeff= 0.0001\n",
    "        hidden_layer_choices = {5: (150, 60, 15),}\n",
    "        conv_layer_parameters = [(5,10), (3,5),]\n",
    "        hidden_layers = hidden_layer_choices[5]\n",
    "        \n",
    "        reshaped_input = Reshape(target_shape=(seq_len, num_letters),name='reshaped_input')(sequence_input)\n",
    "        prior_layer = reshaped_input \n",
    "        for idx, (kernel_width, num_filters) in enumerate(conv_layer_parameters):\n",
    "            conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_width, padding='same', name='conv_'+str(idx))(prior_layer) # mimic a kmer\n",
    "            prior_layer = conv_layer\n",
    "        H = Flatten(name='flatten')(prior_layer)\n",
    "        for idx,h in enumerate(hidden_layers): \n",
    "            H = Dropout(dropout_rate, name='dropout_'+str(idx))(H)\n",
    "            H = Dense(h, activation='relu', kernel_regularizer=l2(reg_coeff), name='dense_'+str(idx))(H)\n",
    "        out_onoff = Dense(1,activation=\"linear\",name='on_output')(H)\n",
    "        \n",
    "        predictor_inputs = []\n",
    "        predictor_outputs = [out_onoff]\n",
    "\n",
    "        return predictor_inputs, predictor_outputs, _initialize_predictor_weights\n",
    "\n",
    "    return _load_predictor_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. Set-up gradient ascent workflow. Convert to callable function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants \n",
    "\n",
    "# get seed input which we will modify \n",
    "num_samples = 1\n",
    "\n",
    "# template specifying what to modify and what not (biological constaints)\n",
    "switch = 'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
    "rbs = 'AACAGAGGAGA'\n",
    "start_codon = 'ATG'\n",
    "stem1 = 'NNNNNN'#'XXXXXX'\n",
    "stem2 = 'NNNNNNNNN'#'XXXXXXXXX'\n",
    "\n",
    "bio_constraints = switch + rbs + stem1 + start_codon + stem2 \n",
    "\n",
    "# define target on/off values \n",
    "target_onoff = 1\n",
    "target = [[target_onoff], ] # keep in this format in case you want to adapt for separate on and off predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loss function\n",
    "# ensure biological constraints are satisfied per sequence\n",
    "\n",
    "def stem_base_pairing(pwm): \n",
    "    # ensure that location of 1s in switch region matches reverse complement of stem\n",
    "    \n",
    "    def reverse_complement(base_index): \n",
    "        # ACGT = alphabett\n",
    "        if base_index == 0: return 3\n",
    "        elif base_index == 1: return 2 \n",
    "        elif base_index == 2: return 1 \n",
    "        elif base_index == 3: return 0\n",
    "    \n",
    "    # reverse complement is reverse over axis of one-hot encoded nt \n",
    "    nt_reversed = K.reverse(pwm, axes=2)\n",
    "    stem1_score = 6 - K.sum(pwm[:, 24, :, 0]*nt_reversed[:, 41,:, 0] + pwm[:, 25, :, 0]*nt_reversed[:, 42, :, 0]+ pwm[:,26, :, 0]*nt_reversed[:, 43, :, 0] + pwm[:, 27, :, 0]*nt_reversed[:, 44, :, 0] + pwm[:, 28, :, 0]*nt_reversed[:, 45, :, 0]+ pwm[:, 29, :, 0]*nt_reversed[:, 46, :, 0])\n",
    "    stem2_score = 9 - K.sum(pwm[:, 12, :, 0]*nt_reversed[:, 50, :, 0] + pwm[:, 13, :, 0]*nt_reversed[:, 51, :, 0]+ pwm[:, 14, :, 0]*nt_reversed[:, 52, :, 0]+ pwm[:, 15, :, 0]*nt_reversed[:, 53, :, 0] + pwm[:, 16, :, 0]*nt_reversed[:, 54, :, 0] + pwm[:, 17, :, 0]*nt_reversed[:,55, :, 0]+ pwm[:, 18,:, 0]*nt_reversed[:, 56, :, 0] + pwm[:, 19, :, 0]*nt_reversed[:,57, :, 0] + pwm[:, 20, :, 0]*nt_reversed[:, 58, :, 0])\n",
    "    return 10*stem1_score + 10*stem2_score\n",
    "\n",
    "def loss_func(predictor_outputs) :\n",
    "    pwm_logits, pwm, sampled_pwm, predicted_out = predictor_outputs\n",
    "  \n",
    "    #Create target constant -- want predicted value for modified input to be close to target input \n",
    "    target_out = K.tile(K.constant(target), (K.shape(sampled_pwm)[0], 1))\n",
    "    target_cost = (target_out - predicted_out)**2\n",
    "    print(target_out, target_cost, predicted_out)\n",
    "    base_pairing_cost = stem_base_pairing(sampled_pwm)\n",
    "    print(base_pairing_cost)\n",
    "    print(K.mean(target_cost + base_pairing_cost, axis=-1))\n",
    "    \n",
    "    ## use this return statement to include the basepairing cost\n",
    "    #return K.mean(target_cost + base_pairing_cost, axis=-1)\n",
    "    \n",
    "    ## use this return statement to ignore the basepairing cost\n",
    "    # modifying so we don't have the base pairing constraint- will make sure complementary after\n",
    "    return K.mean(target_cost, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_ascent(input_toehold_seq, original_out):\n",
    "\n",
    "    # build generator network\n",
    "    _, seqprop_generator = build_generator(seq_length=seq_len, n_sequences=num_samples, batch_normalize_pwm=True,init_sequences = [input_toehold_seq],\n",
    "                                          sequence_templates=bio_constraints)# batch_normalize_pwm=True)\n",
    "    \n",
    "    # build predictor network and hook it on the generator PWM output tensor\n",
    "    _, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(final_model_path), n_sequences=num_samples, eval_mode='pwm')\n",
    "\n",
    "    #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "    _, loss_model = build_loss_model(seqprop_predictor, loss_func, )\n",
    "\n",
    "    #Specify Optimizer to use\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    #Compile Loss Model (Minimize self)\n",
    "    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "    #Fit Loss Model\n",
    "    #seed_input = np.reshape([X[0]], [1,59,4,1]) # any input toehold to be modified\n",
    "\n",
    "    callbacks =[\n",
    "                EarlyStopping(monitor='loss', min_delta=0.001, patience=5, verbose=0, mode='auto'),\n",
    "                #SeqPropMonitor(predictor=seqprop_predictor)#, plot_every_epoch=True, track_every_step=True, )#cse_start_pos=70, isoform_start=target_cut, isoform_end=target_cut+1, pwm_start=70-40, pwm_end=76+50, sequence_template=sequence_template, plot_pwm_indices=[0])\n",
    "            ]\n",
    "\n",
    "    num_epochs=50\n",
    "    train_history = loss_model.fit([], np.ones((1, 1)), epochs=num_epochs, steps_per_epoch=1000, callbacks=callbacks)\n",
    "\n",
    "    #Retrieve optimized PWMs and predicted (optimized) target\n",
    "    _, optimized_pwm, optimized_onehot, predicted_out = seqprop_predictor.predict(x=None, steps=1)\n",
    "    print('Original ON/OFF:', original_out)\n",
    "    print('Predicted ON/OFF: ', predicted_out)\n",
    "    \n",
    "    return optimized_pwm, optimized_onehot, predicted_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7. Run gradient ascent on the specified seed inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_onehot(oh_seq): \n",
    "    return ''.join(alph[idx] for idx in np.argmax(oh_seq,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/seqprop-0.1-py3.7.egg/seqprop/generator/seqprop_generator.py:167: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/seqprop-0.1-py3.7.egg/seqprop/generator/seqprop_generator.py:169: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
      "\n",
      "Tensor(\"lambda_1/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_1/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_2/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_1/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_1/Mean:0\", shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:From /anaconda2/envs/clean_toehold_venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5214\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 749us/step - loss: 0.4040\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 945us/step - loss: 0.2548\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 838us/step - loss: 0.1136\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 754us/step - loss: 0.0871\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 762us/step - loss: 0.0826\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 771us/step - loss: 0.0800\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 818us/step - loss: 0.0804\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 779us/step - loss: 0.0803\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 775us/step - loss: 0.0804\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 813us/step - loss: 0.0791\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 797us/step - loss: 0.0800\n",
      "Original ON/OFF: [-0.01647474]\n",
      "Predicted ON/OFF:  [[0.73908037]]\n",
      "Tensor(\"lambda_2/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_2/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_4/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_2/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_2/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5094\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 895us/step - loss: 0.4541\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 773us/step - loss: 0.4444\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 891us/step - loss: 0.3849\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 841us/step - loss: 0.2441\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 822us/step - loss: 0.0448\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 836us/step - loss: 0.0286\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 0.0234\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 749us/step - loss: 0.0191\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 854us/step - loss: 0.0192\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 960us/step - loss: 0.0182\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 766us/step - loss: 0.0169\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 811us/step - loss: 0.0167\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 774us/step - loss: 0.0175\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 896us/step - loss: 0.0160\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 747us/step - loss: 0.0164\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 801us/step - loss: 0.0162\n",
      "Original ON/OFF: [-0.01783934]\n",
      "Predicted ON/OFF:  [[0.97627103]]\n",
      "Tensor(\"lambda_3/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_3/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_6/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_3/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_3/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5240\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 886us/step - loss: 0.4009\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 838us/step - loss: 0.2503\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 833us/step - loss: 0.1133\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 826us/step - loss: 0.0857\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 824us/step - loss: 0.0811\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 844us/step - loss: 0.0805\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 864us/step - loss: 0.0817\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 834us/step - loss: 0.0795\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 850us/step - loss: 0.0784\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 818us/step - loss: 0.0792\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 816us/step - loss: 0.0770\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 839us/step - loss: 0.0798\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 884us/step - loss: 0.0783\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 796us/step - loss: 0.0798\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 898us/step - loss: 0.0806\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0801\n",
      "Original ON/OFF: [-0.01647474]\n",
      "Predicted ON/OFF:  [[0.73927546]]\n",
      "Tensor(\"lambda_4/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_4/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_8/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_4/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_4/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5074\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 851us/step - loss: 0.4609\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 845us/step - loss: 0.4465\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 884us/step - loss: 0.4248\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 873us/step - loss: 0.3109\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 856us/step - loss: 0.1108\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 944us/step - loss: 0.0339\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 946us/step - loss: 0.0239\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0202\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 975us/step - loss: 0.0182\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 971us/step - loss: 0.0188\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 897us/step - loss: 0.0173\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 996us/step - loss: 0.0173\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0156\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 927us/step - loss: 0.0165\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0161\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0163\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 912us/step - loss: 0.0157\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s 858us/step - loss: 0.0168\n",
      "Original ON/OFF: [-0.01783934]\n",
      "Predicted ON/OFF:  [[0.9797836]]\n",
      "Tensor(\"lambda_5/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_5/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_10/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_5/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_5/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5200\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3962\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2501\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 902us/step - loss: 0.1137\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 921us/step - loss: 0.0874\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 895us/step - loss: 0.0808\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 937us/step - loss: 0.0810\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 914us/step - loss: 0.0795\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 924us/step - loss: 0.0807\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 912us/step - loss: 0.0802\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 948us/step - loss: 0.0789\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0777\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.0795\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 923us/step - loss: 0.0795\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 928us/step - loss: 0.0810\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0789\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0778\n",
      "Original ON/OFF: [-0.01647474]\n",
      "Predicted ON/OFF:  [[0.7391342]]\n",
      "Tensor(\"lambda_6/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_6/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_12/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_6/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_6/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5105\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4480\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4420\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 939us/step - loss: 0.3682\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2228\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0542\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0327\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0262\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0197\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0178\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 994us/step - loss: 0.0175\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0165\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0170\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 993us/step - loss: 0.0162\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0155\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0152\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0167\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0154\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0155\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0162\n",
      "Original ON/OFF: [-0.01783934]\n",
      "Predicted ON/OFF:  [[0.97992676]]\n",
      "Tensor(\"lambda_7/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_7/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_14/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_7/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_7/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5232\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4020\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2561\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1177\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0861\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0816\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0845\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0790\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0775\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0813\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0760\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0785\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0797\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0773\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0782\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0785\n",
      "Original ON/OFF: [-0.01647474]\n",
      "Predicted ON/OFF:  [[0.73894185]]\n",
      "Tensor(\"lambda_8/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_8/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_16/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_8/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_8/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5097\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4478\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4426\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3826\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2408\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0799\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0316\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0250\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0186\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0182\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0177\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0181\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0162\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0168\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0167\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0156A:\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0162\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0166\n",
      "Original ON/OFF: [-0.01783934]\n",
      "Predicted ON/OFF:  [[0.98494196]]\n",
      "Tensor(\"lambda_9/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_9/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_18/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_9/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_9/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5214\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3904\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2412\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1096\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0872\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0814\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0820\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0788\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0807\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0807\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0810\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0778\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0790\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0809\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0814\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0798\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0807\n",
      "Original ON/OFF: [-0.01647474]\n",
      "Predicted ON/OFF:  [[0.7383546]]\n",
      "Tensor(\"lambda_10/Tile:0\", shape=(1, 1), dtype=float32) Tensor(\"lambda_10/pow:0\", shape=(1, 1), dtype=float32) Tensor(\"on_output_20/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"lambda_10/add_13:0\", shape=(), dtype=float32)\n",
      "Tensor(\"lambda_10/Mean:0\", shape=(1,), dtype=float32)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5135\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4539\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4444\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4021\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2675\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1096\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0368\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0272\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0203\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0170\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0161\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0157\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0153\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0149\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0153\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0148\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0149\n",
      "Original ON/OFF: [-0.01783934]\n",
      "Predicted ON/OFF:  [[0.9738625]]\n"
     ]
    }
   ],
   "source": [
    "optimized_pwms = [] # store the probabilities\n",
    "optimized_seqs = [] # store the converted sequences to be tested \n",
    "predicted_targets = [] # store the original and predicted target values \n",
    "\n",
    "# run 5 optimization rounds for each sequence- part of STORM algorithm\n",
    "num_of_optimization_rounds = 5\n",
    "for i in range(0, num_of_optimization_rounds):\n",
    "    for idx, (toehold_seq, original_out) in enumerate(zip(toehold_seqs, y)): \n",
    "        optimized_pwm, optimized_onehot, predicted_out = run_gradient_ascent(toehold_seq, original_out)\n",
    "        optimized_pwms.append(np.reshape(optimized_pwm, [59, 4]))\n",
    "        predicted_targets.append(predicted_out)\n",
    "        new_seq = invert_onehot(np.reshape(optimized_onehot, [59,4]))\n",
    "        optimized_seqs.append(new_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8. Change toeholds to adhere to basepairing and toehold structure- post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame()\n",
    "data_df['old_switches'] = list(itertools.chain.from_iterable(itertools.repeat(x, num_of_optimization_rounds) for x in seqs))\n",
    "data_df['old_predicted_onoff'] = list(itertools.chain.from_iterable(itertools.repeat(x, num_of_optimization_rounds) for x in storm_pred_onoff_vals))\n",
    "data_df['new_switch'] = optimized_seqs\n",
    "data_df['predicted_onoff'] = predicted_targets\n",
    "data_df['optimized_pwm'] = optimized_pwms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbs = 'AACAGAGGAGA'\n",
    "start_codon = 'ATG'\n",
    "\n",
    "# Make function to generate reverse compliment of the DNA strand\n",
    "def make_rev_complement(string):\n",
    "    new_str = ''\n",
    "    for s in string:\n",
    "        char = ''\n",
    "        if s == 'A':\n",
    "            char = 'T'\n",
    "        elif s == 'T':\n",
    "            char = 'A'\n",
    "        elif s == 'C':\n",
    "            char = 'G'\n",
    "        elif s == 'G':\n",
    "            char = 'C'\n",
    "        else:\n",
    "            print('UH OH! Character not A, T, C, or G')\n",
    "        new_str += char\n",
    "    new_str = new_str[::-1]\n",
    "    return new_str\n",
    "\n",
    "# Make function to check for stop codons\n",
    "def check_for_stop(toehold): \n",
    "    stop_codons = ['TAG', 'TAA', 'TGA']\n",
    "    location_of_start = 47\n",
    "    search1 = toehold.find(stop_codons[0]) == location_of_start\n",
    "    search2 = toehold.find(stop_codons[1]) == location_of_start\n",
    "    search3 = toehold.find(stop_codons[2]) == location_of_start\n",
    "    return (search1 | search2  | search3)\n",
    "\n",
    "# Make function to actually turn trigger into toehold\n",
    "def turn_switch_to_toehold(switch):\n",
    "    stem1 = make_rev_complement(switch[24:30])\n",
    "    stem2 = make_rev_complement(switch[12:21])\n",
    "    toehold = switch + rbs + stem1 + start_codon + stem2\n",
    "    return toehold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rev comp\n",
    "def check_rev_comp(full_59nt):\n",
    "    stem1 = make_rev_complement(full_59nt[24:30])\n",
    "    stem2 = make_rev_complement(full_59nt[12:21])\n",
    "    stem1_comp = full_59nt[41:47]\n",
    "    stem2_comp = full_59nt[50:59]\n",
    "    \n",
    "    return((stem1 == stem1_comp) and (stem2 == stem2_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rbs and start codon are unchanged\n",
    "def check_rbs_and_start(full_59nt):\n",
    "    rbs_exists = (full_59nt[30:41] == rbs)\n",
    "    start_exists = (full_59nt[47:50] == start_codon)\n",
    "    return(rbs_exists and start_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CGCCAACAAAGATGTCACATTATAATATCGAACAGAGGAGACGATATATGAATGTGACA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCTCGTCTACCCCTTCAAACAGAGGAGATGAAGGATGAGACGAGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CGCCAACAAAGATGTCACATTATAATATCGAACAGAGGAGACGATATATGAATGTGACA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCGCGTCTACCCCTCCAAACAGAGGAGATGGAGGATGAGACGCGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CGCCAACAAAGATGTCACATTATAATATCGAACAGAGGAGACGATATATGAATGTGACA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCGCGTCTACGCCTCCAAACAGAGGAGATGGAGGATGAGACGCGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CGCCAACAAAGATGTCACATTATAATATCGAACAGAGGAGACGATATATGAATGTGACA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "TTCATTATTATCTGCGCGTCTACGCCTCCAAACAGAGGAGATGGAGGATGAGACGCGCA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CGCCAACAAAGATGTCACATTATAATATCGAACAGAGGAGACGATATATGAATGTGACA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n",
      "checking for rev comp:  False\n",
      "checking for rbs and start codon:  True\n",
      "CGCCATAATAAATGTCCGGCTATCACTTCAAACAGAGGAGATGAAGTATGAGCCGGACA\n",
      "checking for rev comp:  True\n",
      "checking for rbs and start codon:  True\n"
     ]
    }
   ],
   "source": [
    "# convert new switches to bp complementarity / toehold structure\n",
    "new_fixed_switches = []\n",
    "for toehold in data_df['new_switch']:\n",
    "    base_30nt = toehold[0:30]\n",
    "    print('checking for rev comp: ', check_rev_comp(toehold))\n",
    "    print('checking for rbs and start codon: ', check_rbs_and_start(toehold))\n",
    "    new_toehold = turn_switch_to_toehold(base_30nt)\n",
    "    print(new_toehold)\n",
    "    print('checking for rev comp: ', check_rev_comp(new_toehold))\n",
    "    print('checking for rbs and start codon: ', check_rbs_and_start(new_toehold))\n",
    "    new_fixed_switches.append(new_toehold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['NEW_fixed_switch'] = new_fixed_switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([_get_one_hot_encoding(s) for s in new_fixed_switches]).astype(np.float32)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "#print(predictions)\n",
    "\n",
    "data_df['NEW_onoff_preds'] = np.reshape(predictions, [num_seqs*num_of_optimization_rounds,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_seqs = pd.DataFrame()\n",
    "onoff_col = data_df.columns.get_loc(\"NEW_onoff_preds\")\n",
    "\n",
    "# cull so we have just the best out of each 5\n",
    "for i in range(0, num_seqs):\n",
    "    start = i * num_of_optimization_rounds\n",
    "    end = start + num_of_optimization_rounds\n",
    "    best_toehold_so_far = data_df.iloc[start,:]\n",
    "    for j in range(start+1, end):\n",
    "        curr_toehold = data_df.iloc[j,:]\n",
    "        if (data_df.iloc[j, onoff_col] > data_df.iloc[start, onoff_col]):\n",
    "            best_toehold_so_far = curr_toehold\n",
    "    best_seqs = pd.concat([best_seqs, best_toehold_so_far], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_switches</th>\n",
       "      <th>old_predicted_onoff</th>\n",
       "      <th>new_switch</th>\n",
       "      <th>predicted_onoff</th>\n",
       "      <th>optimized_pwm</th>\n",
       "      <th>NEW_fixed_switch</th>\n",
       "      <th>NEW_onoff_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACAAAAAAACAATAAAAAATAGAGAAAAAGAACAGAGGAGACTTTT...</td>\n",
       "      <td>[-0.01647474]</td>\n",
       "      <td>CGCCAACAAAGATGTCACATTATAATATCGAACAGAGGAGACGATA...</td>\n",
       "      <td>[[0.73908037]]</td>\n",
       "      <td>[[0.000115180126, 0.9996408, 9.35327e-05, 0.00...</td>\n",
       "      <td>CGCCAACAAAGATGTCACATTATAATATCGAACAGAGGAGACGATA...</td>\n",
       "      <td>0.726936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ATAAACAAAATGGATATTATAGACAAAAAAAACAGAGGAGATTTTT...</td>\n",
       "      <td>[-0.01783934]</td>\n",
       "      <td>CGCCATAATAAATGTCCGGCTATCACTTCAAACAGAGGAGATGAGG...</td>\n",
       "      <td>[[0.9738625]]</td>\n",
       "      <td>[[0.00020086991, 0.99918264, 0.00018441, 0.000...</td>\n",
       "      <td>CGCCATAATAAATGTCCGGCTATCACTTCAAACAGAGGAGATGAAG...</td>\n",
       "      <td>0.682196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        old_switches old_predicted_onoff  \\\n",
       "0  ACAAAAAAACAATAAAAAATAGAGAAAAAGAACAGAGGAGACTTTT...       [-0.01647474]   \n",
       "9  ATAAACAAAATGGATATTATAGACAAAAAAAACAGAGGAGATTTTT...       [-0.01783934]   \n",
       "\n",
       "                                          new_switch predicted_onoff  \\\n",
       "0  CGCCAACAAAGATGTCACATTATAATATCGAACAGAGGAGACGATA...  [[0.73908037]]   \n",
       "9  CGCCATAATAAATGTCCGGCTATCACTTCAAACAGAGGAGATGAGG...   [[0.9738625]]   \n",
       "\n",
       "                                       optimized_pwm  \\\n",
       "0  [[0.000115180126, 0.9996408, 9.35327e-05, 0.00...   \n",
       "9  [[0.00020086991, 0.99918264, 0.00018441, 0.000...   \n",
       "\n",
       "                                    NEW_fixed_switch NEW_onoff_preds  \n",
       "0  CGCCAACAAAGATGTCACATTATAATATCGAACAGAGGAGACGATA...        0.726936  \n",
       "9  CGCCATAATAAATGTCCGGCTATCACTTCAAACAGAGGAGATGAAG...        0.682196  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_seqs = best_seqs.transpose()\n",
    "best_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change name if you would like\n",
    "out_dir = 'data/gradient_ascent_sequences/'\n",
    "best_seqs.to_csv(out_dir + 'worst_toeholds_optimized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_toehold_venv",
   "language": "python",
   "name": "clean_toehold_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
